{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Sony Semiconductor Solutions Corp. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import json\n",
    "import jsonschema\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configurations\n",
    "\n",
    "Load the configuration file and set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_symlink(path: pathlib.Path):\n",
    "    if path.is_symlink():\n",
    "        msg = \"Symbolic link is not supported. Please use real folder or file\"\n",
    "        raise OSError(errno.ELOOP, f\"{msg}\", f\"{path}\")\n",
    "\n",
    "\n",
    "configuration_path = pathlib.Path(\"./configuration.json\")\n",
    "validate_symlink(configuration_path)\n",
    "\n",
    "with open(configuration_path, \"r\") as f:\n",
    "    app_configuration = json.load(f)\n",
    "\n",
    "configuration_schema_path = pathlib.Path(\"./configuration_schema_convert.json\")\n",
    "validate_symlink(configuration_schema_path)\n",
    "\n",
    "with open(configuration_schema_path, \"r\") as f:\n",
    "    json_schema = json.load(f)\n",
    "\n",
    "# Validate configuration.\n",
    "jsonschema.validate(app_configuration, json_schema)\n",
    "\n",
    "dataset_conversion_base_file = app_configuration[\n",
    "    \"dataset_conversion_base_file\"\n",
    "].replace(os.path.sep, \"/\")\n",
    "validate_symlink(pathlib.Path(dataset_conversion_base_file))\n",
    "dataset_conversion_dir = app_configuration[\"dataset_conversion_dir\"].replace(\n",
    "    os.path.sep, \"/\"\n",
    ")\n",
    "validate_symlink(pathlib.Path(dataset_conversion_dir))\n",
    "dataset_conversion_validation_split = app_configuration[\n",
    "    \"dataset_conversion_validation_split\"\n",
    "]\n",
    "dataset_conversion_seed = app_configuration[\"dataset_conversion_seed\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dataset for transfer learning / quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const datas\n",
    "cvat_exported = \"cvat_exported\"\n",
    "train = \"training\"\n",
    "validation = \"validation\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract exported dataset file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file = pathlib.Path(dataset_conversion_dir)\n",
    "if not os.path.exists(p_file):\n",
    "    os.makedirs(p_file)\n",
    "\n",
    "existslist = []\n",
    "existslist.append(os.path.exists(pathlib.Path(dataset_conversion_dir + \"/labels.json\")))\n",
    "existslist.append(os.path.exists(pathlib.Path(dataset_conversion_dir + \"/training/\")))\n",
    "existslist.append(os.path.exists(pathlib.Path(dataset_conversion_dir + \"/validation/\")))\n",
    "existslist.append(\n",
    "    os.path.exists(pathlib.Path(dataset_conversion_dir + \"/cvat_exported/\"))\n",
    ")\n",
    "\n",
    "if any(existslist):\n",
    "    msg = (\n",
    "        \"The dataset_conversion_dir already contains the dataset. \"\n",
    "        \"Please remove the dataset or set another directory: \"\n",
    "    )\n",
    "    raise FileExistsError(f\"{msg}{dataset_conversion_dir}\")\n",
    "\n",
    "exported_path = os.path.join(dataset_conversion_dir, cvat_exported)\n",
    "\n",
    "shutil.unpack_archive(dataset_conversion_base_file, exported_path)\n",
    "\n",
    "print(f\"extracted to {exported_path}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Labels Info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(exported_path)\n",
    "files_dir = sorted([f for f in files if os.path.isdir(os.path.join(exported_path, f))])\n",
    "dict = {files_dir[i]: (i) for i in range(0, len(files_dir))}\n",
    "\n",
    "with open(dataset_conversion_dir + \"/labels.json\", \"w\") as f:\n",
    "    json.dump(dict, f, ensure_ascii=False)\n",
    "\n",
    "print(\"labels.json generated.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate dataset for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    exported_path,\n",
    "    validation_split=dataset_conversion_validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=dataset_conversion_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    exported_path,\n",
    "    validation_split=dataset_conversion_validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=dataset_conversion_seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy dataset images to separated directory of training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_class_names = np.array(train_ds.class_names)\n",
    "trn_path = os.path.join(dataset_conversion_dir, train)\n",
    "train_files = sorted(train_ds.file_paths)\n",
    "if not os.path.exists(trn_path):\n",
    "    os.makedirs(trn_path)\n",
    "for _class_name in trn_class_names:\n",
    "    _class_path = os.path.join(trn_path, _class_name)\n",
    "    if not os.path.exists(_class_path):\n",
    "        os.makedirs(_class_path)\n",
    "\n",
    "for _file in train_files:\n",
    "    shutil.copyfile(_file, _file.replace(cvat_exported, train))\n",
    "\n",
    "print(f\"{len(train_files)} {train} files copied.\")\n",
    "\n",
    "val_class_names = np.array(val_ds.class_names)\n",
    "val_path = os.path.join(dataset_conversion_dir, validation)\n",
    "val_files = sorted(val_ds.file_paths)\n",
    "if not os.path.exists(val_path):\n",
    "    os.makedirs(val_path)\n",
    "for _class_name in val_class_names:\n",
    "    _class_path = os.path.join(val_path, _class_name)\n",
    "    if not os.path.exists(_class_path):\n",
    "        os.makedirs(_class_path)\n",
    "\n",
    "for _file in val_files:\n",
    "    shutil.copyfile(_file, _file.replace(cvat_exported, validation))\n",
    "\n",
    "print(f\"{len(val_files)} {validation} files copied.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
