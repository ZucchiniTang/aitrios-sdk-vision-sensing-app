{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022-2023 Sony Semiconductor Solutions Corp. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize Model (Image Classification Keras Model)\n",
    "\n",
    "This notebook explains the workflow to quantize custom AI model using [\"**Model Compression Toolkit (MCT)**\"](https://github.com/sony/model_optimization).\n",
    "\n",
    "Instructions are described in [README.md](./README.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import glob\n",
    "import json\n",
    "import jsonschema\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import model_compression_toolkit as mct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from model_compression_toolkit import FolderImageLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configurations\n",
    "\n",
    "Load the configuration file and set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_symlink(path: pathlib.Path):\n",
    "    if path.is_symlink():\n",
    "        raise OSError(\n",
    "            errno.ELOOP,\n",
    "            \"Symbolic link is not supported. Please use real folder or file\",\n",
    "            f\"{path}\",\n",
    "        )\n",
    "\n",
    "\n",
    "configuration_path = pathlib.Path(\"./configuration.json\")\n",
    "validate_symlink(configuration_path)\n",
    "\n",
    "with open(configuration_path, \"r\") as f:\n",
    "    app_configuration = json.load(f)\n",
    "\n",
    "configuration_schema_path = pathlib.Path(\"./configuration_schema.json\")\n",
    "validate_symlink(configuration_schema_path)\n",
    "\n",
    "with open(configuration_schema_path, \"r\") as f:\n",
    "    json_schema = json.load(f)\n",
    "\n",
    "# Validate configuration.\n",
    "jsonschema.validate(app_configuration, json_schema)\n",
    "\n",
    "source_keras_model = app_configuration[\"source_keras_model\"].replace(os.path.sep, \"/\")\n",
    "validate_symlink(pathlib.Path(source_keras_model))\n",
    "\n",
    "dataset_image_dir = app_configuration[\"dataset_image_dir\"].replace(os.path.sep, \"/\")\n",
    "validate_symlink(pathlib.Path(dataset_image_dir))\n",
    "\n",
    "batch_size = int(app_configuration[\"batch_size\"])\n",
    "\n",
    "input_tensor_size = int(app_configuration[\"input_tensor_size\"])\n",
    "\n",
    "iteration_count = int(app_configuration[\"iteration_count\"])\n",
    "\n",
    "output_dir = app_configuration[\"output_dir\"].replace(os.path.sep, \"/\")\n",
    "validate_symlink(pathlib.Path(output_dir))\n",
    "\n",
    "evaluate_image_dir = app_configuration[\"evaluate_image_dir\"].replace(os.path.sep, \"/\")\n",
    "validate_symlink(pathlib.Path(evaluate_image_dir))\n",
    "\n",
    "evaluate_image_extension = \"*.\" + app_configuration[\"evaluate_image_extension\"]\n",
    "\n",
    "evaluate_label_file = app_configuration[\"evaluate_label_file\"].replace(os.path.sep, \"/\")\n",
    "validate_symlink(pathlib.Path(evaluate_label_file))\n",
    "\n",
    "evaluate_result_dir = app_configuration[\"evaluate_result_dir\"].replace(os.path.sep, \"/\")\n",
    "validate_symlink(pathlib.Path(evaluate_result_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(source_keras_model):\n",
    "    # earlier style keras h5 file\n",
    "    keras_model = load_model(source_keras_model)\n",
    "else:\n",
    "    # later style keras SavedModel folder\n",
    "    keras_model = tf.keras.models.load_model(source_keras_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Keras Model\n",
    "\n",
    "Quantize the Keras model using \"**MCT**\" with JPEG images for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for calibration (resize and normalization).\n",
    "# The implementation depends on AI model's preprocessing of learning.\n",
    "# The default implementation is for MobileNetV1.\n",
    "# Please change the implementation according to the using AI model.\n",
    "# The input_tensor_size must match the input size of the AI model.\n",
    "# RESIZE_SCALE must be greater than or equal to 1.\n",
    "\n",
    "MEAN = 127.5\n",
    "STD = 127.5\n",
    "RESIZE_SCALE = 256 / input_tensor_size\n",
    "SIZE = input_tensor_size\n",
    "\n",
    "\n",
    "def resize(x):\n",
    "    resize_side = max(\n",
    "        RESIZE_SCALE * SIZE / x.shape[0], RESIZE_SCALE * SIZE / x.shape[1]\n",
    "    )\n",
    "    height_tag = int(np.round(resize_side * x.shape[0]))\n",
    "    width_tag = int(np.round(resize_side * x.shape[1]))\n",
    "    resized_img = cv2.resize(x, (width_tag, height_tag))\n",
    "    offset_height = int((height_tag - SIZE) / 2)\n",
    "    offset_width = int((width_tag - SIZE) / 2)\n",
    "    cropped_img = resized_img[\n",
    "        offset_height : offset_height + SIZE, offset_width : offset_width + SIZE\n",
    "    ]\n",
    "    return cropped_img\n",
    "\n",
    "\n",
    "def normalization(x):\n",
    "    return (x - MEAN) / STD\n",
    "\n",
    "\n",
    "# Create a representative data generator, which returns a list of images.\n",
    "# The images can be preprocessed using a list of preprocessing functions.\n",
    "image_data_loader = FolderImageLoader(\n",
    "    dataset_image_dir, preprocessing=[resize, normalization], batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Callable representative dataset for calibration purposes.\n",
    "# The function must be called without any arguments, and must return a list numpy arrays\n",
    "# (array for each model's input).\n",
    "def representative_data_gen() -> list:\n",
    "    return [image_data_loader.sample()]\n",
    "\n",
    "\n",
    "# Quantize a model using the representative_data_gen as the calibration images.\n",
    "# Set the number of calibration iterations.\n",
    "quantized_keras_model, quantization_info = mct.keras_post_training_quantization(\n",
    "    keras_model, representative_data_gen, n_iter=iteration_count\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from Keras to TFLite Model\n",
    "\n",
    "Quantized TFLite model will be saved as **`model_quantized.tflite`** in **`output_dir`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_quantized = tf.lite.TFLiteConverter.from_keras_model(quantized_keras_model)\n",
    "converter_quantized.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_quantized.inference_input_type = tf.uint8\n",
    "tflite_model_quantized = converter_quantized.convert()\n",
    "\n",
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file_quantized = tflite_models_dir / \"model_quantized.tflite\"\n",
    "tflite_model_file_quantized.write_bytes(tflite_model_quantized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-quantized TFLite model will be saved as **`model.tflite`** in **`output_dir`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir / \"model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Evaluation\n",
    "\n",
    "Load label ids. The json file format is that key as label name and value as label id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(evaluate_label_file) as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumerate JPEG images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n",
    "\n",
    "\n",
    "files_all = sorted(\n",
    "    glob.glob(f\"{evaluate_image_dir}/**/{evaluate_image_extension}\", recursive=True),\n",
    "    key=natural_keys,\n",
    ")\n",
    "\n",
    "if len(files_all) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Image for evaluation not found in the evaluate_image_dir: {evaluate_image_dir}\"\n",
    "    )\n",
    "\n",
    "folders = sorted(\n",
    "    glob.glob(f\"{evaluate_image_dir}/*/\", recursive=True), key=natural_keys\n",
    ")\n",
    "\n",
    "if len(folders) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Folder as label for evaluation not found in the evaluate_image_dir: {evaluate_image_dir}\"\n",
    "    )\n",
    "\n",
    "# get images and ground truth for evaluation\n",
    "test_images = []\n",
    "ground_truth_ids = []\n",
    "for folder in folders:\n",
    "    files_in_folder = sorted(\n",
    "        glob.glob(os.path.join(folder, evaluate_image_extension), recursive=True),\n",
    "        key=natural_keys,\n",
    "    )\n",
    "    for file in files_in_folder:\n",
    "        label = os.path.basename(os.path.dirname(file))\n",
    "        if label in labels:\n",
    "            label_id = labels[label]\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                f\"Label name: {label} not found in the {evaluate_label_file}\"\n",
    "            )\n",
    "        filename = os.path.basename(file)\n",
    "        info = dict()\n",
    "        info[\"path\"] = file\n",
    "        info[\"imageID\"] = filename\n",
    "        test_images.append(info)\n",
    "        ground_truth_ids.append(label_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluate methods for TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(interpreter, images):\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    # refs: https://github.com/tensorflow/models\n",
    "    #       /research/slim/preprocessing/inception_preprocessing.py\n",
    "    predictions = []\n",
    "    for test_image in tqdm(images):\n",
    "        if input_details[\"dtype\"] == np.uint8:\n",
    "            # Pre-processing: add batch dimension and convert to uint8 to match with\n",
    "            # the model's input data format.\n",
    "            image = tf.io.decode_jpeg(tf.io.read_file(test_image[\"path\"]), channels=3)\n",
    "            # image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "            image = tf.expand_dims(image, 0)\n",
    "            image = tf.compat.v1.image.resize_bilinear(\n",
    "                image, [input_tensor_size, input_tensor_size], align_corners=False\n",
    "            )\n",
    "            image = tf.cast(image, tf.uint8)\n",
    "            interpreter.set_tensor(input_details[\"index\"], image)\n",
    "        else:\n",
    "            # for non-quantized model.\n",
    "            image = tf.io.decode_jpeg(tf.io.read_file(test_image[\"path\"]), channels=3)\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "            # image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "            image = tf.expand_dims(image, 0)\n",
    "            image = tf.compat.v1.image.resize_bilinear(\n",
    "                image, [input_tensor_size, input_tensor_size], align_corners=False\n",
    "            )\n",
    "            image = tf.subtract(image, 0.5)\n",
    "            image = tf.multiply(image, 2.0)\n",
    "            interpreter.set_tensor(input_details[\"index\"], image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        output = interpreter.tensor(output_details[\"index\"])\n",
    "        prediction = dict()\n",
    "        prediction[\"imageID\"] = test_image[\"imageID\"]\n",
    "        prediction[\"predictions\"] = [np.argmax(output()[0])]\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    accurate_count = 0\n",
    "    for index in range(len(predictions)):\n",
    "        if predictions[index][\"predictions\"][0] == ground_truth_ids[index]:\n",
    "            accurate_count += 1\n",
    "    top_1_accuracy = accurate_count * 1.0 / len(predictions)\n",
    "\n",
    "    return top_1_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluate methods for Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_keras_model(model, images, ground_truth):\n",
    "    def load_image(image_path):\n",
    "        image = tf.io.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        # image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.compat.v1.image.resize_bilinear(\n",
    "            image, [input_tensor_size, input_tensor_size], align_corners=False\n",
    "        )\n",
    "        image = tf.squeeze(image, [0])\n",
    "        return image\n",
    "\n",
    "    image_paths = []\n",
    "    for test_image in images:\n",
    "        image_paths.append(test_image[\"path\"])\n",
    "    images_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        [str(path) for path in image_paths]\n",
    "    ).map(load_image)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        np.array(ground_truth).astype(np.uint32)\n",
    "    )\n",
    "    test_data = tf.data.Dataset.zip((images_ds, labels_ds)).shuffle(len(image_paths))\n",
    "\n",
    "    model.trainable = False\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    test_result = model.evaluate(test_data.batch(1))\n",
    "\n",
    "    return test_result[1]  # Top1 accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load non-quantized TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir / \"model.tflite\"\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load quantized TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file_quantized = tflite_models_dir / \"model_quantized.tflite\"\n",
    "\n",
    "interpreter_quantized = tf.lite.Interpreter(model_path=str(tflite_model_file_quantized))\n",
    "interpreter_quantized.allocate_tensors()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Evaluate non-quantized tflite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_tflite = evaluate_tflite_model(interpreter, test_images)\n",
    "print(f\"Top1 accuracy: non-quantized tflite: {top_1_accuracy_tflite}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate quantized tflite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_tflite_quantized = evaluate_tflite_model(\n",
    "    interpreter_quantized, test_images\n",
    ")\n",
    "print(f\"Top1 accuracy: quantized tflite: {top_1_accuracy_tflite_quantized}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate non-quantized Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_keras = evaluate_keras_model(keras_model, test_images, ground_truth_ids)\n",
    "print(f\"\\nTop1 accuracy: non-quantized keras: {top_1_accuracy_keras}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate quantized Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_keras_quantized = evaluate_keras_model(\n",
    "    quantized_keras_model, test_images, ground_truth_ids\n",
    ")\n",
    "print(f\"\\nTop1 accuracy: quantized keras: {top_1_accuracy_keras_quantized}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [top_1_accuracy_tflite],\n",
    "        [top_1_accuracy_tflite_quantized],\n",
    "        [top_1_accuracy_keras],\n",
    "        [top_1_accuracy_keras_quantized],\n",
    "    ],\n",
    "    index=[\n",
    "        \"non-quantized tflite (model.tflite)\",\n",
    "        \"quantized tflite (model_quantized.tflite)\",\n",
    "        \"non-quantized keras\",\n",
    "        \"quantized keras\",\n",
    "    ],\n",
    "    columns=[\"Top1 accuracy\"],\n",
    ")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save evaluation results as **`results.json`** in **`evaluate_result_dir`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_output_dir = pathlib.Path(evaluate_result_dir)\n",
    "evaluate_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with open(evaluate_output_dir / \"results.json\", \"w\") as f:\n",
    "    results = dict()\n",
    "    results[\"top_1_accuracy_keras\"] = top_1_accuracy_keras\n",
    "    results[\"top_1_accuracy_keras_quantized\"] = top_1_accuracy_keras_quantized\n",
    "    results[\"top_1_accuracy_tflite\"] = top_1_accuracy_tflite\n",
    "    results[\"top_1_accuracy_tflite_quantized\"] = top_1_accuracy_tflite_quantized\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
