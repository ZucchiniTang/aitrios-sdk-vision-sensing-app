{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Sony Semiconductor Solutions Corp. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize Model (Image Classification Keras Model)\n",
    "\n",
    "This notebook explains the workflow to quantize custom AI model using [Model Compression Toolkit (MCT)](https://github.com/sony/model_optimization).\n",
    "\n",
    "Instructions are described in [README.md](./README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import model_compression_toolkit as mct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from model_compression_toolkit import FolderImageLoader\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configurations\n",
    "\n",
    "Load the configuration file and set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./configuration.json\", \"r\") as f:\n",
    "    app_configuration = json.load(f)\n",
    "\n",
    "source_keras_model = app_configuration.get(\"source_keras_model\", \"\")\n",
    "\n",
    "dataset_image_dir = app_configuration[\"dataset_image_dir\"]\n",
    "\n",
    "batch_size = app_configuration[\"batch_size\"]\n",
    "\n",
    "input_tensor_size = app_configuration[\"input_tensor_size\"]\n",
    "\n",
    "iteration_count = app_configuration[\"iteration_count\"]\n",
    "\n",
    "output_dir = app_configuration[\"output_dir\"]\n",
    "\n",
    "evaluate_image_dir = app_configuration[\"evaluate_image_dir\"]\n",
    "evaluate_image_extension = '*.' + app_configuration[\"evaluate_image_extension\"]\n",
    "evaluate_ground_truth_file = app_configuration[\"evaluate_ground_truth_file\"]\n",
    "evaluate_result_dir = app_configuration[\"evaluate_result_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not source_keras_model:\n",
    "    raise FileNotFoundError(\n",
    "        f'Please set \"source_keras_model\" value in configuration.json file.')\n",
    "else:\n",
    "    if os.path.isfile(source_keras_model):\n",
    "        # earlier style keras h5 file\n",
    "        keras_model = load_model(source_keras_model)\n",
    "    else:\n",
    "        # later style keras Saved Model folder\n",
    "        keras_model = tf.keras.models.load_model(source_keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Keras Model\n",
    "\n",
    "Quantize the Keras model using MCT with jpeg images for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for calibration (resize and normalization).\n",
    "# The implementation depends on AI model's preprocessing of learning.\n",
    "# The default implementation is for MobileNetV1.\n",
    "# Please change the implementation according to the using AI model.\n",
    "\n",
    "MEAN = 127.5\n",
    "STD = 127.5\n",
    "RESIZE_SCALE = 256 / input_tensor_size\n",
    "SIZE = input_tensor_size\n",
    "\n",
    "def resize(x):\n",
    "    resize_side = max(RESIZE_SCALE * SIZE / x.shape[0], RESIZE_SCALE * SIZE / x.shape[1])\n",
    "    height_tag = int(np.round(resize_side * x.shape[0]))\n",
    "    width_tag = int(np.round(resize_side * x.shape[1]))\n",
    "    resized_img = cv2.resize(x, (width_tag, height_tag))\n",
    "    offset_height = int((height_tag - SIZE) / 2)\n",
    "    offset_width = int((width_tag - SIZE) / 2)\n",
    "    cropped_img = resized_img[offset_height:offset_height + SIZE,\n",
    "                              offset_width:offset_width + SIZE]\n",
    "    return cropped_img\n",
    "\n",
    "def normalization(x):\n",
    "    return (x - MEAN) / STD\n",
    "\n",
    "# Create a representative data generator, which returns a list of images.\n",
    "# The images can be preprocessed using a list of preprocessing functions.\n",
    "image_data_loader = FolderImageLoader(dataset_image_dir,\n",
    "                                      preprocessing=[resize, normalization],\n",
    "                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Callable representative dataset for calibration purposes.\n",
    "# The function must be called without any arguments, and must return a list numpy arrays\n",
    "# (array for each model's input).\n",
    "def representative_data_gen() -> list:\n",
    "    return [image_data_loader.sample()]\n",
    "\n",
    "# Quantize a model using the representative_data_gen as the calibration images.\n",
    "# Set the number of calibration iterations.\n",
    "quantized_keras_model, quantization_info = mct.keras_post_training_quantization(\n",
    "    keras_model,\n",
    "    representative_data_gen,\n",
    "    n_iter=iteration_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from Keras to TFLite Model\n",
    "\n",
    "Quantized TFLite model will be saved as **`model_quantized.tflite`** in **`output_dir`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_quantized = tf.lite.TFLiteConverter.from_keras_model(quantized_keras_model)\n",
    "converter_quantized.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_quantized.inference_input_type = tf.uint8\n",
    "tflite_model_quantized = converter_quantized.convert()\n",
    "\n",
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file_quantized = tflite_models_dir/\"model_quantized.tflite\"\n",
    "tflite_model_file_quantized.write_bytes(tflite_model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-quantized TFLite model will be saved as **`model.tflite`** in **`output_dir`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir/\"model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Evaluation\n",
    "\n",
    "Load ground truth. The file format is that class ids are listed per row in the same order as the image filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(evaluate_ground_truth_file) as f:\n",
    "    ground_truth_ids = np.array([s.strip() for s in f.readlines()], dtype='uint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumerate jpeg images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "files = sorted(glob.glob(str(pathlib.Path(evaluate_image_dir)/evaluate_image_extension)),\n",
    "               key=natural_keys)\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f'Image for evaluation not found in the evaluate_image_dir: {evaluate_image_dir}')\n",
    "\n",
    "# get images for evaluation\n",
    "test_images = []\n",
    "for idx, file in enumerate(files):\n",
    "    filename = os.path.basename(file)\n",
    "    info = dict()\n",
    "    info['path'] = file\n",
    "    info['imageID'] = filename\n",
    "    test_images.append(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluate methods for TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(interpreter, images):\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    # refs: https://github.com/tensorflow/models\n",
    "    #       /research/slim/preprocessing/inception_preprocessing.py\n",
    "    predictions = []\n",
    "    for test_image in tqdm(images):\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            # Pre-processing: add batch dimension and convert to uint8 to match with\n",
    "            # the model's input data format.\n",
    "            image = tf.io.decode_jpeg(tf.io.read_file(test_image['path']), channels=3)\n",
    "            # image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "            image = tf.expand_dims(image, 0)\n",
    "            image = tf.compat.v1.image.resize_bilinear(image, [input_tensor_size, input_tensor_size],\n",
    "                                                       align_corners=False)\n",
    "            image = tf.cast(image, tf.uint8)\n",
    "            interpreter.set_tensor(input_details[\"index\"], image)\n",
    "        else:\n",
    "            # for non-quantized model.\n",
    "            image = tf.io.decode_jpeg(tf.io.read_file(test_image['path']), channels=3)\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "            # image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "            image = tf.expand_dims(image, 0)\n",
    "            image = tf.compat.v1.image.resize_bilinear(image, [input_tensor_size, input_tensor_size],\n",
    "                                                       align_corners=False)\n",
    "            image = tf.subtract(image, 0.5)\n",
    "            image = tf.multiply(image, 2.0)\n",
    "            interpreter.set_tensor(input_details[\"index\"], image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        output = interpreter.tensor(output_details[\"index\"])\n",
    "        prediction = dict()\n",
    "        prediction['imageID'] = test_image['imageID']\n",
    "        prediction['predictions'] = [np.argmax(output()[0])]\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    accurate_count = 0\n",
    "    for index in range(len(predictions)):\n",
    "        if predictions[index]['predictions'][0] == ground_truth_ids[index]:\n",
    "            accurate_count += 1\n",
    "    top_1_accuracy = accurate_count * 1.0 / len(predictions)\n",
    "\n",
    "    return top_1_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluate methods for Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_keras_model(data_dir, gt_file, model):\n",
    "    with open(gt_file, 'r') as fp:\n",
    "        labels = [line.strip() for line in fp.readlines()]\n",
    "\n",
    "    def load_image(image_path):\n",
    "        image = tf.io.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        # image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.compat.v1.image.resize_bilinear(image, [input_tensor_size, input_tensor_size],\n",
    "                                                   align_corners=False)\n",
    "        image = tf.squeeze(image, [0])\n",
    "        return image\n",
    "\n",
    "    def atoi(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "\n",
    "    def natural_keys(text):\n",
    "        return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "    image_paths = sorted(\n",
    "        glob.glob(str(pathlib.Path(evaluate_image_dir)/evaluate_image_extension)),\n",
    "        key=natural_keys)\n",
    "    image_paths = list(image_paths)\n",
    "    images_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        [str(path) for path in image_paths]).map(load_image)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(np.array(labels).astype(np.uint32))\n",
    "    test_data = tf.data.Dataset.zip((images_ds, labels_ds)).shuffle(len(image_paths))\n",
    "\n",
    "    model.trainable = False\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    model.summary()\n",
    "\n",
    "    test_result = model.evaluate(test_data.batch(1))\n",
    "\n",
    "    return test_result[1]  # Top1 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load non-quantized TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir/\"model.tflite\"\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load quantized TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(output_dir)\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file_quantized = tflite_models_dir/\"model_quantized.tflite\"\n",
    "\n",
    "interpreter_quantized = tf.lite.Interpreter(model_path=str(tflite_model_file_quantized))\n",
    "interpreter_quantized.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Evaluate non-quantized tflite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_tflite = evaluate_tflite_model(interpreter, test_images)\n",
    "print(f'Top1 accuracy: non-quantized tflite: {top_1_accuracy_tflite}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate quantized tflite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_tflite_quantized = evaluate_tflite_model(interpreter_quantized, test_images)\n",
    "print(f'Top1 accuracy: quantized tflite: {top_1_accuracy_tflite_quantized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate non-quantized Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_keras = evaluate_keras_model(evaluate_image_dir,\n",
    "                                            evaluate_ground_truth_file,\n",
    "                                            keras_model)\n",
    "print(f'\\nTop1 accuracy: non-quantized keras: {top_1_accuracy_keras}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate quantized Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy_keras_quantized = evaluate_keras_model(evaluate_image_dir,\n",
    "                                                      evaluate_ground_truth_file,\n",
    "                                                      quantized_keras_model)\n",
    "print(f'\\nTop1 accuracy: quantized keras: {top_1_accuracy_keras_quantized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[top_1_accuracy_tflite],\n",
    "                   [top_1_accuracy_tflite_quantized],\n",
    "                   [top_1_accuracy_keras],\n",
    "                   [top_1_accuracy_keras_quantized]],\n",
    "                  index=['non-quantized tflite (model.tflite)',\n",
    "                         'quantized tflite (model_quantized.tflite)',\n",
    "                         'non-quantized keras',\n",
    "                         'quantized keras'],\n",
    "                  columns=['Top1 accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save evaluation results as **`results.json`** in **`evaluate_output_dir`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_output_dir = pathlib.Path(evaluate_result_dir)\n",
    "evaluate_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with open(evaluate_output_dir/\"results.json\", 'w') as f:\n",
    "    results = dict()\n",
    "    results['top_1_accuracy_keras'] = top_1_accuracy_keras\n",
    "    results['top_1_accuracy_keras_quantized'] = top_1_accuracy_keras_quantized\n",
    "    results['top_1_accuracy_tflite'] = top_1_accuracy_tflite\n",
    "    results['top_1_accuracy_tflite_quantized'] = top_1_accuracy_tflite_quantized\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
