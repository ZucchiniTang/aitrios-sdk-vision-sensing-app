{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Sony Semiconductor Solutions Corp. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision and Sensing Application SDK in a day\n",
    "\n",
    "Using [\"**Zone Detection**\"](https://developer.aitrios.sony-semicon.com/development-guides/tutorials/sample-application/) sample application of tutorials in AITRIOS developer site as an example,\n",
    "\n",
    "this jupyter notebook demonstrates an overview of the \"**Vision and Sensing Application SDK**\" in a day.\n",
    "\n",
    "You can run this jupyter notebook step-by-step (one cell a time).\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> TensorFlow and some library may print warning logs in output cell. Please ignore them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI model dependent defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must not change following defines.\n",
    "\n",
    "# for AI model trained in this notebook using TensorFlow Object Detection API\n",
    "INPUT_SIZE = 300\n",
    "DNN_OUTPUT_DETECTIONS = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"**Vision and Sensing Application**\" dependent defines\n",
    "\n",
    "You can customize following **`MAX_DETECTIONS`** and **`APP_VERSION_NUMBER`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision and Sensing Application can use this parameter to limit the the number of objects\n",
    "# detected by AI model in an image. In this case, if AI model detects more than 10 obejects,\n",
    "# Vision and Sensing Application outputs only 10 objects which has higher precision.\n",
    "MAX_DETECTIONS = 10\n",
    "\n",
    "# version number of Vision and Sensing Application for Zone Detection\n",
    "APP_VERSION_NUMBER = \"01.01.00\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook dependent defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must not change following defines. You can skip reading them to understand the SDK.\n",
    "\n",
    "SDK_ENV_ROOT_DIR = \"../..\"\n",
    "\n",
    "VNSAPP_CODE_CLONE_DIR = (\n",
    "    f\"{SDK_ENV_ROOT_DIR}\"\n",
    "    \"/.devcontainer/dependencies/aitrios-sdk-zone-detection-webapp-cs\"\n",
    ")\n",
    "VNSAPP_CODE_DIR = \"sample\"\n",
    "VNSAPP_DIR = \"application\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common function\n",
    "\n",
    "This cell defines a general utility function. So you can skip reading it to understand the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shell_command(command: str):\n",
    "    \"\"\"Run shell command with output log and checking return code.\"\"\"\n",
    "\n",
    "    with subprocess.Popen(\n",
    "        command,\n",
    "        stdout=subprocess.PIPE,\n",
    "        shell=True,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        bufsize=1,\n",
    "        close_fds=True,\n",
    "    ) as process:\n",
    "        # for line in iter(process.stdout.readline, b\"\"):\n",
    "        while True:\n",
    "            line = process.stdout.readline()\n",
    "            if line:\n",
    "                print(line.rstrip().decode(\"utf-8\"))\n",
    "            if process.poll() is not None:\n",
    "                break\n",
    "        process.stdout.close()\n",
    "        return_code = process.wait()\n",
    "        if return_code != 0:\n",
    "            raise subprocess.CalledProcessError(process.returncode, process.args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Dataset for training AI model in this notebook is pre-installed in the SDK.\n",
    "\n",
    "If you want to annotate image by yourself, please refer [../../tutorials/2_prepare_dataset/annotate_images/object_detection/README.md](../../tutorials/2_prepare_dataset/annotate_images/object_detection/README.md) for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must not change following defines. You can skip reading them to understand the SDK.\n",
    "\n",
    "IMAGES_TRAINING_DIR = \"./dataset/images/training\"\n",
    "IMAGES_VALIDATION_DIR = \"./dataset/images/validation\"\n",
    "IMAGES_ORG_DIR = \"./dataset/images_org\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract images from dataset for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (Path(IMAGES_VALIDATION_DIR).is_dir() and Path(IMAGES_TRAINING_DIR).is_dir()):\n",
    "    # Extract images\n",
    "    run_shell_command(\n",
    "        f\"mkdir -p {IMAGES_ORG_DIR} \\\n",
    "            && unzip -o -j -d {IMAGES_ORG_DIR} \\\n",
    "            {VNSAPP_CODE_CLONE_DIR}/{VNSAPP_CODE_DIR}/training_images.zip\"\n",
    "    )\n",
    "\n",
    "    # split training and validation images\n",
    "    IMAGES_VALIDATION_FILES = \"training_image_010.jpg training_image_020.jpg \\\n",
    "        training_image_025.jpg training_image_037.jpg training_image_045.jpg \\\n",
    "        training_image_052.jpg training_image_060.jpg\"\n",
    "    run_shell_command(\n",
    "        f\"mkdir -p {IMAGES_TRAINING_DIR} && mkdir -p {IMAGES_VALIDATION_DIR}\"\n",
    "    )\n",
    "    run_shell_command(\n",
    "        f\"cd {IMAGES_ORG_DIR} \\\n",
    "            && mv {IMAGES_VALIDATION_FILES} ../../{IMAGES_VALIDATION_DIR}/ \\\n",
    "            && mv ./* ../../{IMAGES_TRAINING_DIR}/\"\n",
    "    )\n",
    "    run_shell_command(f\"rm -d {IMAGES_ORG_DIR}\")\n",
    "    print(f\"Images extracted in {IMAGES_TRAINING_DIR} and {IMAGES_VALIDATION_DIR}\")\n",
    "else:\n",
    "    print(\n",
    "        f\"Already extracted images in {IMAGES_TRAINING_DIR} and {IMAGES_VALIDATION_DIR}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an image for evaluating (used as Input Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_INPUT = f\"{IMAGES_VALIDATION_DIR}/training_image_037.jpg\"\n",
    "image = cv2.cvtColor(cv2.imread(IMAGE_INPUT), cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, dsize=(INPUT_SIZE, INPUT_SIZE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare AI model\n",
    "\n",
    "In this notebook, AI model is created and trained using TensorFlow Object Detection API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defines\n",
    "\n",
    "Training AI model takes several minutes to execute.\n",
    "\n",
    "So if following **`is_always_retraining`** is False, training AI model executes only at first time.\n",
    "\n",
    "If you want to train second time, please set **`is_always_retraining`** True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_always_retraining = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must not change following defines. You can skip reading them to understand the SDK.\n",
    "\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "TRAINING_DOCKER_IMAGE_NAME = \"tf1_od_api_env:1.0.0\"\n",
    "TRAINING_DOCKER_VOLUME_DIR = \"/root/samples/zone_detection\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment for transfer learning (based on SSD MobileNet V1 AI model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download base AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(f\"./{MODELS_DIR}/out/ckpt\").is_dir():\n",
    "    BASE_AI_MODEL_FILE = \"ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18\"\n",
    "    run_shell_command(\n",
    "        f\"wget \\\n",
    "            -q http://download.tensorflow.org/models/object_detection/{BASE_AI_MODEL_FILE}.tar.gz \\\n",
    "            -P ./{MODELS_DIR}/\"\n",
    "    )\n",
    "    run_shell_command(f\"mkdir -p ./{MODELS_DIR}/out\")\n",
    "    run_shell_command(\n",
    "        f\"tar -xvf ./{MODELS_DIR}/{BASE_AI_MODEL_FILE}.tar.gz -C ./{MODELS_DIR}/out\"\n",
    "    )\n",
    "    run_shell_command(\n",
    "        f\"mv ./{MODELS_DIR}/out/{BASE_AI_MODEL_FILE} ./{MODELS_DIR}/out/ckpt\"\n",
    "    )\n",
    "    run_shell_command(f\"rm ./{MODELS_DIR}/{BASE_AI_MODEL_FILE}.tar.gz\")\n",
    "    print(f\"Base AI model downloaded in ./{MODELS_DIR}/out/ckpt\")\n",
    "else:\n",
    "    print(f\"Already downloaded base AI model in ./{MODELS_DIR}/out/ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build docker image for learning AI model using TensorFlow Object Detection API\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> If the docker image isn't cached, it takes about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_shell_command(\n",
    "    f\"docker build \\\n",
    "        --build-arg UBUNTU_VERSION=18.04 \\\n",
    "        --build-arg PIP=pip3 \\\n",
    "        --build-arg PYTHON=python3 \\\n",
    "        --build-arg USE_PYTHON_3_NOT_2=1 \\\n",
    "        --build-arg _PY_SUFFIX=3 \\\n",
    "        --build-arg TF_PACKAGE=tensorflow \\\n",
    "        --build-arg TF_PACKAGE_VERSION=1.15.5 \\\n",
    "        . -f Dockerfile \\\n",
    "        -t {TRAINING_DOCKER_IMAGE_NAME} --network host\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy of base model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common functions\n",
    "\n",
    "These cells define general utility function. So you can skip reading them to understand the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(output_file: str, graph_def_file: str):\n",
    "    \"\"\"Convert SavedModel to TFLite within training docker container.\"\"\"\n",
    "\n",
    "    output_arrays = (\n",
    "        \"TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,\"\n",
    "        \"TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\"\n",
    "    )\n",
    "\n",
    "    run_shell_command(\n",
    "        f'docker run --rm -t -v $(pwd):{TRAINING_DOCKER_VOLUME_DIR} {TRAINING_DOCKER_IMAGE_NAME} \\\n",
    "            tflite_convert \\\n",
    "        --output_file={output_file} \\\n",
    "        --graph_def_file={graph_def_file} \\\n",
    "        --inference_type=QUANTIZED_UINT8 \\\n",
    "        --input_arrays=\"normalized_input_image_tensor\" \\\n",
    "        --output_arrays={output_arrays} \\\n",
    "        --mean_values=128 \\\n",
    "        --std_dev_values=128 \\\n",
    "        --input_shapes=1,{INPUT_SIZE},{INPUT_SIZE},3 \\\n",
    "        --change_concat_input_ranges=false \\\n",
    "        --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
    "        --allow_custom_ops'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteInterpreter:\n",
    "    \"\"\"A class to evaluate TFLite.\"\"\"\n",
    "\n",
    "    def __init__(self, model_file: str):\n",
    "        self.interpreter = TFLiteInterpreter._make_interpreter(model_file)\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_interpreter(model_file: str) -> Any:\n",
    "        model_file, *device = model_file.split(\"@\")\n",
    "        interpreter = tf.lite.Interpreter(model_path=model_file)\n",
    "        interpreter.allocate_tensors()\n",
    "        return interpreter\n",
    "\n",
    "    def _prepare_input(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        if self.input_details[0][\"dtype\"] in [np.uint8, np.int8]:\n",
    "            input_scale, input_zero_point = self.input_details[0][\"quantization\"]\n",
    "            input_batch = input_batch / input_scale + input_zero_point\n",
    "        return tf.cast(\n",
    "            np.expand_dims(input_batch, axis=0), self.input_details[0][\"dtype\"]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_output(interpreter, output_details, index):\n",
    "        output = interpreter.get_tensor(output_details[index][\"index\"])\n",
    "        if output_details[index][\"dtype\"] in [np.uint8, np.int8]:\n",
    "            scale, zero_point = output_details[index][\"quantization\"]\n",
    "            output = scale * (output - zero_point)\n",
    "        return tf.cast(output, output_details[index][\"dtype\"])\n",
    "\n",
    "    def _get_outputs(self) -> List[np.ndarray]:\n",
    "        return [\n",
    "            self._get_output(self.interpreter, self.output_details, index)\n",
    "            for index in range(len(self.output_details))\n",
    "        ]\n",
    "\n",
    "    def run(self, input_batch: Any) -> None:\n",
    "        self.interpreter.set_tensor(\n",
    "            self.input_details[0][\"index\"], self._prepare_input(input_batch)\n",
    "        )\n",
    "        self.interpreter.invoke()\n",
    "        return self._get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_display_tflite(\n",
    "    model_path: str, image: cv2.Mat, score_threshold: float, dnn_output_bboxes: int\n",
    ") -> Tuple[list, list, cv2.Mat]:\n",
    "    \"\"\"Evaluate TFLite, convert Output Tensor format from TFLite to Edge AI Device and display\n",
    "    the image overlaid with Output Tensor.\"\"\"\n",
    "\n",
    "    interp = TFLiteInterpreter(model_path)\n",
    "\n",
    "    raw_output_tensors = interp.run(image.astype(np.float32) / 255.0)\n",
    "    output_tensor = np.concatenate([np.array(x).flatten() for x in raw_output_tensors])\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    num_of_detection = int(output_tensor[len(output_tensor) - 1])\n",
    "\n",
    "    postprocessed = []\n",
    "    for index in range(num_of_detection):\n",
    "        inference = []\n",
    "        inference.append(output_tensor[4 * dnn_output_bboxes + index])  # cls\n",
    "        inference.append(output_tensor[5 * dnn_output_bboxes + index])  # score\n",
    "        inference.append(output_tensor[0 + 4 * index])  # ymin\n",
    "        inference.append(output_tensor[1 + 4 * index])  # xmin\n",
    "        inference.append(output_tensor[2 + 4 * index])  # ymax\n",
    "        inference.append(output_tensor[3 + 4 * index])  # xmax\n",
    "        postprocessed.append(inference)\n",
    "\n",
    "    # float array of Output Tensor on Edge AI Device\n",
    "    output_tensor_device = [0.0 for _ in range(dnn_output_bboxes * 6 + 1)]\n",
    "    output_tensor_device[dnn_output_bboxes * 6] = len(postprocessed)  # num of detection\n",
    "\n",
    "    overlaid_text_image = np.zeros((INPUT_SIZE, INPUT_SIZE, 4), dtype=np.uint8)\n",
    "    result_image = image.copy()\n",
    "    result_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2RGBA)\n",
    "    for idx, x in enumerate(postprocessed):\n",
    "        cls, score, ymin, xmin, ymax, xmax = x\n",
    "        print(f\"score: {score}, class id: {int(cls)}\")\n",
    "\n",
    "        output_tensor_device[dnn_output_bboxes * 0 + idx] = float(ymin)\n",
    "        output_tensor_device[dnn_output_bboxes * 1 + idx] = float(xmin)\n",
    "        output_tensor_device[dnn_output_bboxes * 2 + idx] = float(ymax)\n",
    "        output_tensor_device[dnn_output_bboxes * 3 + idx] = float(xmax)\n",
    "        output_tensor_device[dnn_output_bboxes * 4 + idx] = float(cls)\n",
    "        output_tensor_device[dnn_output_bboxes * 5 + idx] = float(score)\n",
    "\n",
    "        ymin = int(ymin * INPUT_SIZE)\n",
    "        xmin = int(xmin * INPUT_SIZE)\n",
    "        ymax = int(ymax * INPUT_SIZE)\n",
    "        xmax = int(xmax * INPUT_SIZE)\n",
    "        if score > score_threshold:\n",
    "            cv2.rectangle(\n",
    "                result_image, (xmin, ymin), (xmax, ymax), (255, 255, 0, 255), 2\n",
    "            )\n",
    "            text_pos_x = xmin + 2\n",
    "            text_pos_y = ymax - 2\n",
    "            if text_pos_x > (INPUT_SIZE - 35):\n",
    "                text_pos_x = INPUT_SIZE - 35\n",
    "            elif text_pos_x < 2:\n",
    "                text_pos_x = 2\n",
    "            if text_pos_y < 14:\n",
    "                text_pos_y = 14\n",
    "            elif text_pos_y > (INPUT_SIZE - 2):\n",
    "                text_pos_y = INPUT_SIZE - 2\n",
    "            cv2.putText(\n",
    "                overlaid_text_image,\n",
    "                str(\"{0:.0f}%\".format(score * 100.0)),\n",
    "                (text_pos_x, text_pos_y),\n",
    "                cv2.FONT_HERSHEY_PLAIN,\n",
    "                1,\n",
    "                (255, 255, 0, 255),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "    cv2.add(result_image, overlaid_text_image, result_image)\n",
    "    plt.imshow(result_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    return output_tensor_device, postprocessed, result_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert base AI model from SavedModel to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(f\"{MODELS_DIR}/base_model_quantized_od.tflite\").is_file():\n",
    "    OUTPUT_FILE = (\n",
    "        f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/base_model_quantized_od.tflite\"\n",
    "    )\n",
    "    GRAPH_DEF_FILE = (\n",
    "        f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/out/ckpt/tflite_graph.pb\"\n",
    "    )\n",
    "    convert_to_tflite(OUTPUT_FILE, GRAPH_DEF_FILE)\n",
    "    print(\n",
    "        f\"Base AI model TFLite converted in ./{MODELS_DIR}/base_model_quantized_od.tflite\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Already converted base AI model TFLite in ./{MODELS_DIR}/base_model_quantized_od.tflite\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and display TFLite base AI model\n",
    "\n",
    "Since the AI model has not undergone transfer learning, the accurary will be low and cars will not be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"./{MODELS_DIR}/base_model_quantized_od.tflite\"\n",
    "SCORE_THRESHOLD = 0.3\n",
    "output_tensor_device, postprocessed, result_image = evaluate_and_display_tflite(\n",
    "    MODEL, image, SCORE_THRESHOLD, DNN_OUTPUT_DETECTIONS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Using TensorFlow Object Detection API, Quantization Aware Training (QAT) is executed.\n",
    "\n",
    "So the transfer learned AI model is already quantized as is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute transfer learning\n",
    "\n",
    "For customizing transfer learning parameters, you can change **`NUM_TRAIN_STEPS`** in following cell and **`batch_size`** in [./models/pipeline.config](./models/pipeline.config).\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> It takes about 8 minutes.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> The training and validation TFRecord dataset is located in [./dataset/training](./dataset/training) and [./dataset/validation](./dataset/validation) and used in [./models/pipeline.config](./models/pipeline.config).\n",
    ">\n",
    "> The TFRecord dataset is created by CVAT annotation using [./dataset/images/training](./dataset/images/training) and [./dataset/images/validation](./dataset/images/validation) images.\n",
    ">\n",
    "> The annotated backuped CVAT project file is located in [./dataset/cvat_backup/](./dataset/cvat_backup).\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> If you want to train again with more training steps using the same dataset as the previous training, please increase **`NUM_TRAIN_STEPS`** from the last time you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_STEPS = 300\n",
    "\n",
    "if (not Path(f\"{MODELS_DIR}/out/train\").is_dir()) or is_always_retraining:\n",
    "    PIPELINE_CONFIG_PATH = f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/pipeline.config\"\n",
    "    MODEL_DIR = f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/out/train\"\n",
    "    SAMPLE_1_OF_N_EVAL_EXAMPLES = 100\n",
    "    run_shell_command(\n",
    "        f\"docker run --rm -t -v $(pwd):{TRAINING_DOCKER_VOLUME_DIR} \\\n",
    "            --network host {TRAINING_DOCKER_IMAGE_NAME} \\\n",
    "            python /tensorflow/models/research/object_detection/model_main.py \\\n",
    "            --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n",
    "            --model_dir={MODEL_DIR} \\\n",
    "            --num_train_steps={NUM_TRAIN_STEPS} \\\n",
    "            --sample_1_of_n_eval_examples={SAMPLE_1_OF_N_EVAL_EXAMPLES} \\\n",
    "            --alsologtostderr\"\n",
    "    )\n",
    "    print(f\"Learned in ./{MODELS_DIR}/out/train\")\n",
    "else:\n",
    "    print(f\"Already learned in ./{MODELS_DIR}/out/train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export AI model as SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(f\"{MODELS_DIR}/out/models\").is_dir() or is_always_retraining:\n",
    "    TRAINED_CHECKPOINT_PREFIX = (\n",
    "        f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/out/train/model.ckpt-{NUM_TRAIN_STEPS}\"\n",
    "    )\n",
    "    OUTPUT_DIRECTORY = f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/out/models\"\n",
    "    run_shell_command(\n",
    "        f\"docker run --rm -t -v $(pwd):{TRAINING_DOCKER_VOLUME_DIR} {TRAINING_DOCKER_IMAGE_NAME} \\\n",
    "            python /tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
    "            --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n",
    "            --trained_checkpoint_prefix={TRAINED_CHECKPOINT_PREFIX} \\\n",
    "            --output_directory={OUTPUT_DIRECTORY} \\\n",
    "            --add_postprocessing_op=true\"\n",
    "    )\n",
    "    print(f\"Saved in ./{MODELS_DIR}/out/models\")\n",
    "else:\n",
    "    print(f\"Already saved in ./{MODELS_DIR}/out/models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert transfer learned SavedModel to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    not Path(f\"{MODELS_DIR}/model_quantized_od.tflite\").is_file()\n",
    "    or is_always_retraining\n",
    "):\n",
    "    OUTPUT_FILE = f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/model_quantized_od.tflite\"\n",
    "    GRAPH_DEF_FILE = (\n",
    "        f\"{TRAINING_DOCKER_VOLUME_DIR}/{MODELS_DIR}/out/models/tflite_graph.pb\"\n",
    "    )\n",
    "    convert_to_tflite(OUTPUT_FILE, GRAPH_DEF_FILE)\n",
    "    print(f\"Converted in ./{MODELS_DIR}/model_quantized_od.tflite\")\n",
    "else:\n",
    "    print(f\"Already converted in ./{MODELS_DIR}/model_quantized_od.tflite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and display TFLite transfer learned AI model\n",
    "\n",
    "Since the AI model has already undergone transfer learning, the accuracy will be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"./{MODELS_DIR}/model_quantized_od.tflite\"\n",
    "SCORE_THRESHOLD = 0.0\n",
    "output_tensor_device, postprocessed, result_image = evaluate_and_display_tflite(\n",
    "    MODEL, image, SCORE_THRESHOLD, DNN_OUTPUT_DETECTIONS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Output Tensor to json file\n",
    "\n",
    "The Output Tensor file will be used as input of \"**Vision and Sensing Application**\" in the SDK environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./{VNSAPP_DIR}/output_tensor.jsonc\", \"w\") as fp:\n",
    "    json.dump(output_tensor_device, fp, indent=4)\n",
    "    print(f\"Output Tensor saved as ./{VNSAPP_DIR}/output_tensor.jsonc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare \"**Vision and Sensing Application**\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is \"**Vision and Sensing Application**\"?\n",
    "\n",
    "\"**Vision and Sensing Application**\" is post-processing application of AI model's Output Tensor running on Edge AI Device.\n",
    "\n",
    "Please see [../../tutorials/4_prepare_application/1_develop/README.md](../../tutorials/4_prepare_application/1_develop/README.md) for details.\n",
    "\n",
    "In \"**Vision and Sensing Application**\" for \"**Zone Detection**\", by calculating IoU (Intersection over Union) in Edge AI Device, there is an advantage that the amount of uploading data to the cloud can be reduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment for developing \"**Vision and Sensing Application**\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Makefile customized for the SDK environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_shell_command(\n",
    "    f\"cp {VNSAPP_DIR}/Makefile_VnS \\\n",
    "        {VNSAPP_CODE_CLONE_DIR}/{VNSAPP_CODE_DIR}/sample/vision_app/single_dnn/zonedetection/\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop and build \"**Vision and Sensing Application**\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wasm():\n",
    "    \"\"\"build Vision and Sensing Application and copy the compiled Wasm file to this notebook's\n",
    "    directory\"\"\"\n",
    "\n",
    "    run_shell_command(f\"{VNSAPP_DIR}/build.sh\")\n",
    "    run_shell_command(\n",
    "        f\"cp -f {VNSAPP_CODE_CLONE_DIR}/{VNSAPP_CODE_DIR}\"\n",
    "        \"/sample/vision_app/single_dnn/zonedetection/build/release/\"\n",
    "        \"vision_app_zonedetection.wasm\"\n",
    "        f\" ./{VNSAPP_DIR}/\"\n",
    "    )\n",
    "    print(f\"\\nCompiled Wasm file path: ./{VNSAPP_DIR}/vision_app_zonedetection.wasm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify the source code (if needed)\n",
    "\n",
    "The source code exists in [./../../.devcontainer/dependencies/aitrios-sdk-zone-detection-webapp-cs/sample](./../../.devcontainer/dependencies/aitrios-sdk-zone-detection-webapp-cs/sample).\n",
    "\n",
    "Please modify the source code if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Wasm\n",
    "\n",
    "\"**Vision and Sensing Application**\" is compiled to Wasm file.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> If docker image for building Wasm doesn't cached, it takes about 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_wasm()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run \"**Vision and Sensing Application**\" with mock\n",
    "\n",
    "The SDK provides the test application for running \"**Vision and Sensing Application**\", that mocks dependency libraries of Edge AI Device firmware.\n",
    "\n",
    "So you can run \"**Vision and Sensing Application**\" in the SDK environment for development and test.\n",
    "\n",
    "Please see [../../tutorials/4_prepare_application/1_develop/README_wasmdebug.md](../../tutorials/4_prepare_application/1_develop/README_wasmdebug.md) for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must not change following defines. You can skip reading them to understand the SDK.\n",
    "\n",
    "PPL_ENV_DIR = f\"{SDK_ENV_ROOT_DIR}/tutorials/4_prepare_application/1_develop\"\n",
    "PPL_ENV_OUTPUT_DIR = \"testapp/build/loader\"\n",
    "PPL_ENV_OUTPUT_FILE = \"serialized_result_01.bin\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions\n",
    "\n",
    "These cells define general utility function. So you can skip reading them to understand the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_deserialized_inference_result(\n",
    "    file_path: str, image: cv2.Mat, ppl_parameter\n",
    "):\n",
    "    \"\"\"Display an image with overlaid metadata of inference result\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        json_load = json.load(f)\n",
    "\n",
    "    detection_list = json_load[\"perception\"][\"object_detection_list\"]\n",
    "\n",
    "    overlaid_text_image = np.zeros((INPUT_SIZE, INPUT_SIZE, 4), dtype=np.uint8)\n",
    "    result_image = image.copy()\n",
    "    result_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2RGBA)\n",
    "    if (\n",
    "        \"zone\" in ppl_parameter\n",
    "        and \"top_left_x\" in ppl_parameter[\"zone\"]\n",
    "        and \"top_left_y\" in ppl_parameter[\"zone\"]\n",
    "        and \"bottom_right_x\" in ppl_parameter[\"zone\"]\n",
    "        and \"bottom_right_y\" in ppl_parameter[\"zone\"]\n",
    "    ):\n",
    "        zone_xmin = ppl_parameter[\"zone\"][\"top_left_x\"]\n",
    "        zone_ymin = ppl_parameter[\"zone\"][\"top_left_y\"]\n",
    "        zone_xmax = ppl_parameter[\"zone\"][\"bottom_right_x\"]\n",
    "        zone_ymax = ppl_parameter[\"zone\"][\"bottom_right_y\"]\n",
    "        cv2.rectangle(\n",
    "            result_image,\n",
    "            (zone_xmin, zone_ymin),\n",
    "            (zone_xmax, zone_ymax),\n",
    "            (255, 0, 0, 255),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    for idx, detection in enumerate(detection_list):\n",
    "        bbox = detection[\"bounding_box\"]\n",
    "        score = detection[\"score\"]\n",
    "        iou = detection[\"iou\"]\n",
    "        zoneflag = detection[\"zoneflag\"]\n",
    "        xmin, ymin, xmax, ymax = bbox.values()\n",
    "        if zoneflag:\n",
    "            rectangle_color = (255, 255, 0, 255)\n",
    "        else:\n",
    "            rectangle_color = (255, 255, 0, 255)\n",
    "        cv2.rectangle(result_image, (xmin, ymin), (xmax, ymax), rectangle_color, 2)\n",
    "\n",
    "        text_pos_x = xmin + 2\n",
    "        text_pos_y = ymin + 13\n",
    "        if text_pos_x > (INPUT_SIZE - 35):\n",
    "            text_pos_x = INPUT_SIZE - 35\n",
    "        elif text_pos_x < 2:\n",
    "            text_pos_x = 2\n",
    "        if text_pos_y < 13:\n",
    "            text_pos_y = 13\n",
    "        elif text_pos_y > (INPUT_SIZE - 15):\n",
    "            text_pos_y = INPUT_SIZE - 15\n",
    "        cv2.putText(\n",
    "            overlaid_text_image,\n",
    "            str(\"{0:.0f}%\".format(iou * 100.0)),\n",
    "            (text_pos_x, text_pos_y),\n",
    "            cv2.FONT_HERSHEY_PLAIN,\n",
    "            1,\n",
    "            (0, 255, 0, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        text_pos_x = xmin + 2\n",
    "        text_pos_y = ymax - 2\n",
    "        if text_pos_x > (INPUT_SIZE - 35):\n",
    "            text_pos_x = INPUT_SIZE - 35\n",
    "        elif text_pos_x < 2:\n",
    "            text_pos_x = 2\n",
    "        if text_pos_y < 26:\n",
    "            text_pos_y = 26\n",
    "        elif text_pos_y > (INPUT_SIZE - 2):\n",
    "            text_pos_y = INPUT_SIZE - 2\n",
    "        cv2.putText(\n",
    "            overlaid_text_image,\n",
    "            str(\"{0:.0f}%\".format(score * 100.0)),\n",
    "            (text_pos_x, text_pos_y),\n",
    "            cv2.FONT_HERSHEY_PLAIN,\n",
    "            1,\n",
    "            (255, 255, 0, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    cv2.add(result_image, overlaid_text_image, result_image)\n",
    "    plt.imshow(result_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_output_tensor(serialized_output_tensor_file_name: str):\n",
    "    \"\"\"Deserialize Output Tensor binary file to json file\"\"\"\n",
    "\n",
    "    run_shell_command(\n",
    "        f\"cp $(pwd)/{VNSAPP_CODE_CLONE_DIR}/{VNSAPP_CODE_DIR}/schema/zonedetection.fbs \\\n",
    "            $(pwd)/deserialize/zonedetection.fbs\"\n",
    "    )\n",
    "    run_shell_command(\n",
    "        f\"cd $(pwd)/deserialize/ \\\n",
    "        && ./binary_to_json.sh zonedetection.fbs {serialized_output_tensor_file_name} ./\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare PPL Parameter and run \"**Vision and Sensing Application**\"\n",
    "\n",
    "PPL Parameter is used for customizing behavior of \"**Vision and Sensing Application**\" by setting parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PPL Parameter and save it as json\n",
    "\n",
    "At first, use following PPL Parameter as provisional values for building and running \"**Vision and Sensing Application**\" in the SDK environment. The **`zone`** value is incorrect and the **`threshold`** value is set too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_parameter_before = {\n",
    "    \"header\": {\n",
    "        \"id\": \"00\",\n",
    "        \"version\": APP_VERSION_NUMBER\n",
    "    },\n",
    "    \"dnn_output_detections\": DNN_OUTPUT_DETECTIONS,\n",
    "    \"max_detections\": MAX_DETECTIONS,\n",
    "    \"mode\": 0,  # mode of \"Zone Detection\" application. 0: detections not filtered by IoU, 1: detections filtered by IoU\n",
    "    \"zone\": {\n",
    "        \"top_left_x\": 10,\n",
    "        \"top_left_y\": 100,\n",
    "        \"bottom_right_x\": 200,\n",
    "        \"bottom_right_y\": 250\n",
    "    },\n",
    "    \"threshold\": {\n",
    "        \"iou\": 0.9,\n",
    "        \"score\": 0.8\n",
    "    },\n",
    "    \"input_width\": INPUT_SIZE,\n",
    "    \"input_height\": INPUT_SIZE\n",
    "}\n",
    "\n",
    "with open(f\"./{VNSAPP_DIR}/ppl_parameter_before.json\", \"w\") as fp:\n",
    "    json.dump(ppl_parameter_before, fp, indent=4)\n",
    "    print(f\"PPL Parameter saved as ./{VNSAPP_DIR}/ppl_parameter_before.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and run \"**Vision and Sensing Application**\" in the SDK with mock library\n",
    "\n",
    "Running \"**Vision and Sensing Application**\" in the SDK with mock library, metadata of inference result serialized by FlatBuffers is generated as a result.\n",
    "\n",
    "To read the result, deserialize the metadata of inference result.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> If docker image for deserialize isn't cached, it takes about 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_wasm()\n",
    "\n",
    "# Run Wasm on the SDK environment\n",
    "run_shell_command(\n",
    "    f\"{VNSAPP_DIR}/start.sh \\\n",
    "                  -o $(pwd)/{VNSAPP_DIR}/output_tensor.jsonc \\\n",
    "                  -p $(pwd)/{VNSAPP_DIR}/ppl_parameter_before.json\"\n",
    ")\n",
    "\n",
    "# Copy metadata of inference result from test application directory to this notebook directory\n",
    "run_shell_command(\n",
    "    f\"mkdir -p $(pwd)/deserialize && \\\n",
    "        cp -f $(pwd)/{PPL_ENV_DIR}/{PPL_ENV_OUTPUT_DIR}/{PPL_ENV_OUTPUT_FILE} \\\n",
    "            $(pwd)/deserialize/ppl_output_before.bin\"\n",
    ")\n",
    "print(\n",
    "    \"\\nSerialized metadata of inference result binary file path: \\\n",
    "        ./deserialize/ppl_output_before.bin\"\n",
    ")\n",
    "\n",
    "# Deserialize\n",
    "BINARY_FILE = \"ppl_output_before.bin\"\n",
    "deserialize_output_tensor(BINARY_FILE)\n",
    "print(\n",
    "    \"\\nDeserialized metadata of inference result json file path: \\\n",
    "        ./deserialize/ppl_output_before.json\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display deserialized metadata of inference result\n",
    "\n",
    "Since the PPL Parameter are provisional values, the result will not be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./deserialize/ppl_output_before.json\"\n",
    "display_deserialized_inference_result(file_path, image, ppl_parameter_before)\n",
    "\n",
    "# Display legend\n",
    "# Yellow rectangle: detected object\n",
    "# Yellow text: inference score of detected object\n",
    "# Red rectangle: zone defined in PPL Parameter\n",
    "# Green text: IoU (Intersection over Union) with detected object and zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify PPL Parameter and run \"**Vision and Sensing Application**\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PPL Parameter and save it as json (second time)\n",
    "\n",
    "Before running following cell, you can customize the PPL Parameter.\n",
    "\n",
    "For example, following **`mode`**, **`zone`** and **`threshold`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_parameter_after = {\n",
    "    \"header\": {\n",
    "        \"id\": \"00\",\n",
    "        \"version\": \"01.01.00\"\n",
    "    },\n",
    "    \"dnn_output_detections\": DNN_OUTPUT_DETECTIONS,\n",
    "    \"max_detections\": MAX_DETECTIONS,\n",
    "    \"mode\": 0,  # mode of \"Zone Detection\" application. 0: detections not filtered by IoU, 1: detections filtered by IoU\n",
    "    \"zone\": {\n",
    "        \"top_left_x\": 10,\n",
    "        \"top_left_y\": 168,\n",
    "        \"bottom_right_x\": 198,\n",
    "        \"bottom_right_y\": 258\n",
    "    },\n",
    "    \"threshold\": {\n",
    "        \"iou\": 0.5,\n",
    "        \"score\": 0.4\n",
    "    },\n",
    "    \"input_width\": INPUT_SIZE,\n",
    "    \"input_height\": INPUT_SIZE\n",
    "}\n",
    "\n",
    "with open(f\"./{VNSAPP_DIR}/ppl_parameter_after.json\", \"w\") as fp:\n",
    "    json.dump(ppl_parameter_after, fp, indent=4)\n",
    "    print(f\"PPL Parameter saved as ./{VNSAPP_DIR}/ppl_parameter_after.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and run \"**Vision and Sensing Application**\" in the SDK with mock library (second time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_wasm()\n",
    "\n",
    "# Run Wasm on the SDK environment\n",
    "run_shell_command(\n",
    "    f\"{VNSAPP_DIR}/start.sh \\\n",
    "        -o $(pwd)/{VNSAPP_DIR}/output_tensor.jsonc \\\n",
    "        -p $(pwd)/{VNSAPP_DIR}/ppl_parameter_after.json\"\n",
    ")\n",
    "\n",
    "# Copy metadata of inference result from test application directory to this notebook directory\n",
    "run_shell_command(\n",
    "    f\"mkdir -p $(pwd)/deserialize && \\\n",
    "        cp -f $(pwd)/{PPL_ENV_DIR}/{PPL_ENV_OUTPUT_DIR}/{PPL_ENV_OUTPUT_FILE} \\\n",
    "            $(pwd)/deserialize/ppl_output_after.bin\"\n",
    ")\n",
    "print(\n",
    "    \"\\nSerialized metadata of inference result binary file path: \\\n",
    "        ./deserialize/ppl_output_after.bin\"\n",
    ")\n",
    "\n",
    "# Deserialize\n",
    "BINARY_FILE = \"ppl_output_after.bin\"\n",
    "deserialize_output_tensor(BINARY_FILE)\n",
    "print(\n",
    "    \"\\nDeserialized metadata of inference result json file path: \\\n",
    "        ./deserialize/ppl_output_after.json\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display deserialized metadata of inference result (second time)\n",
    "\n",
    "Since the PPL Parameter are configured values, the result will be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./deserialize/ppl_output_after.json\"\n",
    "display_deserialized_inference_result(file_path, image, ppl_parameter_after)\n",
    "\n",
    "# Display legend\n",
    "# Yellow rectangle: detected object\n",
    "# Yellow text: inference score of detected object\n",
    "# Red rectangle: zone defined in PPL Parameter\n",
    "# Green text: IoU (Intersection over Union) with detected object and zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy AI model and \"**Vision and Sensing Application**\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload AI model to Azure Blob Storage and generate SAS URL\n",
    "\n",
    "To deploy AI model to your Edge AI Device using this notebook, you have to prepare Azure Blob Storage by yourself.\n",
    "\n",
    "Please upload the AI model [./models/model_quantized_od.tflite](./models/model_quantized_od.tflite) to Azure Blob Storage and generate SAS URL of the uploaded AI model using Azure Blob Storage.\n",
    "\n",
    "About SAS URL, please see [\"**Console User Manual**\"](https://developer.aitrios.sony-semicon.com/en/documents/console-user-manual) for details.\n",
    "\n",
    "To import AI model from local environment, please see [\"**Console User Manual**\"](https://developer.aitrios.sony-semicon.com/en/documents/console-user-manual)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import copy\n",
    "import errno\n",
    "import os\n",
    "\n",
    "import jsonschema\n",
    "import pandas as pd\n",
    "from console_access_library.client import Client\n",
    "from console_access_library.common.config import Config\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must not change following defines. You can skip reading them to understand the SDK.\n",
    "\n",
    "CONNECTION_CONFIG_PATH = (\n",
    "    f\"{SDK_ENV_ROOT_DIR}/tutorials/_common/set_up_console_client/configuration.json\"\n",
    ")\n",
    "CONNECTION_CONFIG_SCHEMA_PATH = (\n",
    "    f\"{SDK_ENV_ROOT_DIR}\"\n",
    "    \"/tutorials/_common/set_up_console_client/configuration_schema.json\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common function\n",
    "\n",
    "This cell defines a general utility function. So you can skip reading it to understand the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_symlink(path: Path):\n",
    "    \"\"\"Validate symbolic link\"\"\"\n",
    "\n",
    "    if path.is_symlink():\n",
    "        raise OSError(\n",
    "            errno.ELOOP,\n",
    "            \"Symbolic link is not supported. Please use real folder or file\",\n",
    "            f\"{path}\",\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare configurations for connection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Configuration file for connection (only at first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = Path(f\"{CONNECTION_CONFIG_PATH}\")\n",
    "if not configuration_path.is_file():\n",
    "    with open(configuration_path, \"w\") as fp:\n",
    "        configuration_connection = {\n",
    "            \"console_endpoint\": \"\",\n",
    "            \"portal_authorization_endpoint\": \"\",\n",
    "            \"client_secret\": \"\",\n",
    "            \"client_id\": \"\"\n",
    "        }\n",
    "        json.dump(configuration_connection, fp, indent=4)\n",
    "    print(f\"./{CONNECTION_CONFIG_PATH} is created.\")\n",
    "else:\n",
    "    print(f\"./{CONNECTION_CONFIG_PATH} already exists.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit Configuration file for connection\n",
    "\n",
    "Edit the parameters in [./../../tutorials/_common/set_up_console_client/configuration.json](../../tutorials/_common/set_up_console_client/configuration.json).\n",
    "\n",
    "The parameters required to run this notebook are :\n",
    "\n",
    "|Setting|Range|Required/Optional|Remarks\n",
    "|:--|:--|:--|:--|\n",
    "|**`console_endpoint`**|String.<br>See NOTE.|Required|Used for Console Access Library API: **`common.config.Config`**\n",
    "|**`portal_authorization_endpoint`**|String.<br>See NOTE.|Required|Used for Console Access Library API: **`common.config.Config`**\n",
    "|**`client_secret`**|String.<br>See NOTE.|Required|Used for Console Access Library API: **`common.config.Config`**\n",
    "|**`client_id`**|String.<br>See NOTE.|Required|Used for Console Access Library API: **`common.config.Config`**\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> See [API Reference](https://developer.aitrios.sony-semicon.com/development-guides/reference/api-references) of Console Access Library for other restrictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configurations for connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = Path(f\"{CONNECTION_CONFIG_PATH}\")\n",
    "validate_symlink(configuration_path)\n",
    "\n",
    "# Load configuration file.\n",
    "with open(configuration_path, \"r\") as f:\n",
    "    json_load = json.load(f)\n",
    "\n",
    "configuration_schema_path = Path(f\"{CONNECTION_CONFIG_SCHEMA_PATH}\")\n",
    "validate_symlink(configuration_schema_path)\n",
    "\n",
    "# Load configuration schema file.\n",
    "with open(configuration_schema_path, \"r\") as f:\n",
    "    json_schema = json.load(f)\n",
    "\n",
    "# Validate configuration.\n",
    "jsonschema.validate(json_load, json_schema)\n",
    "\n",
    "print(f\"./{CONNECTION_CONFIG_PATH} is loaded.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Console Access Library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Console Access Library and get device list with AI model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_devices_and_ai_models(client_obj: Client):\n",
    "    \"\"\"Get device list with AI model info\"\"\"\n",
    "\n",
    "    # Get an instance of device management API\n",
    "    device_management_obj = client_obj.get_device_management()\n",
    "\n",
    "    # Call an API for get device list\n",
    "    try:\n",
    "        response = device_management_obj.get_devices()\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    print(\"Get device list complete.\")\n",
    "\n",
    "    # Create output list\n",
    "    model_lists = []\n",
    "    devices = response.get(\"devices\", [])\n",
    "\n",
    "    for device in devices:\n",
    "        device_id = device.get(\"device_id\", \"\")\n",
    "\n",
    "        model_list = []\n",
    "        model_list.append(device_id)\n",
    "        models = device.get(\"models\", [])\n",
    "        if any(models):\n",
    "            for model in models:\n",
    "                version_id = \"\"\n",
    "                model_version_id = model.get(\"model_version_id\", \"\")\n",
    "                if len(model_version_id) == 0:\n",
    "                    # There is no models deployed on the device\n",
    "                    model_id = \"\"\n",
    "                elif \":\" not in model_version_id:\n",
    "                    # The model deployed to the device does not exist in Console database\n",
    "                    model_id = model_version_id\n",
    "                else:\n",
    "                    # Split string to model_id and version_id\n",
    "                    model_version_id_list = model_version_id.split(\":v\")\n",
    "\n",
    "                    model_id = model_version_id_list[0]\n",
    "                    version_id = model_version_id_list[1]\n",
    "\n",
    "                model_list.append(model_id)\n",
    "                model_list.append(version_id)\n",
    "\n",
    "            # Fill missing elements with empty string\n",
    "            model_list = model_list + [\"\" for x in range(9 - len(model_list))]\n",
    "\n",
    "            model_lists.append(model_list)\n",
    "        else:\n",
    "            # There is no models deployed on the device\n",
    "            model_list.append(\"\")\n",
    "\n",
    "            # Fill missing elements with empty string\n",
    "            model_list = model_list + [\"\" for x in range(9 - len(model_list))]\n",
    "\n",
    "            model_lists.append(model_list)\n",
    "\n",
    "    if len(model_lists) == 0:\n",
    "        raise Exception(\"There is no data in the device list.\")\n",
    "\n",
    "    output_frame = pd.DataFrame(\n",
    "        model_lists,\n",
    "        columns=[\"Device ID\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\"],\n",
    "    )\n",
    "\n",
    "    # Specify \"Device ID\" column as index row\n",
    "    output_frame.set_index(\"Device ID\", inplace=True)\n",
    "\n",
    "    # Set column to 2 rows\n",
    "    double_columns_num = pd.MultiIndex.from_arrays(\n",
    "        [\n",
    "            [\"Model 1\", \" \", \"Model 2\", \" \", \"Model 3\", \" \", \"Model 4\", \" \"],\n",
    "            [\"ID\", \"Version\", \"ID\", \"Version\", \"ID\", \"Version\", \"ID\", \"Version\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Change column\n",
    "    output_frame.columns = double_columns_num\n",
    "\n",
    "    # setting backup\n",
    "    backup_max_rows = pd.options.display.max_rows\n",
    "    # output limit clear\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    display(output_frame)\n",
    "    # setting restore\n",
    "    pd.set_option(\"display.max_rows\", backup_max_rows)\n",
    "\n",
    "\n",
    "print(\"Create access lib client...\")\n",
    "\n",
    "auth_param = {}\n",
    "auth_param[\"console_endpoint\"] = json_load[\"console_endpoint\"]\n",
    "auth_param[\"portal_authorization_endpoint\"] = json_load[\"portal_authorization_endpoint\"]\n",
    "auth_param[\"client_secret\"] = json_load[\"client_secret\"]\n",
    "auth_param[\"client_id\"] = json_load[\"client_id\"]\n",
    "\n",
    "try:\n",
    "    config_obj = Config(**auth_param)\n",
    "except Exception as e:\n",
    "    # EXCEPTION\n",
    "    raise e\n",
    "\n",
    "# Instantiate Console Access Library Client.\n",
    "client_obj = Client(config_obj)\n",
    "\n",
    "# Get device and model information for deploying to device\n",
    "get_devices_and_ai_models(client_obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare configurations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Configuration file (only at first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = Path(\"./configuration.json\")\n",
    "if not configuration_path.is_file():\n",
    "    model_sas_url = \"YOUR_SAS_URL_OF_AI_MODEL\"\n",
    "    model_id = \"YOUR_AI_MODEL_ID\"\n",
    "    # model_version_number = \"1.00\"\n",
    "\n",
    "    ppl_file = f\"{VNSAPP_DIR}/vision_app_zonedetection.wasm\"\n",
    "    app_name = \"YOUR_APP_NAME\"\n",
    "    app_version_number = APP_VERSION_NUMBER\n",
    "\n",
    "    config_id = \"YOUR_CONFIG_ID\"\n",
    "\n",
    "    device_id = \"YOUR_DEVICE_ID\"\n",
    "\n",
    "    command_parameter_file_name = \"command_parameter_file.json\"\n",
    "\n",
    "    with open(configuration_path, \"w\") as fp:\n",
    "        configuration = {\n",
    "            \"import_model\": {\n",
    "                \"model_id\": model_id,\n",
    "                \"model\": model_sas_url,\n",
    "                # \"converted\": false,\n",
    "                \"vendor_name\": \"YOUR_VENDOR_NAME\",\n",
    "                \"comment\": \"YOUR_MODEL_COMMENT\",\n",
    "                # \"network_type\": \"0\",\n",
    "                \"labels\": [\"\"],\n",
    "            },\n",
    "            \"import_app\": {\n",
    "                \"ppl_file\": ppl_file,\n",
    "                \"app_name\": app_name,\n",
    "                \"version_number\": app_version_number,\n",
    "                \"comment\": \"YOUR_APP_COMMENT\",\n",
    "            },\n",
    "            \"deploy_model\": {\n",
    "                \"should_create_deploy_config\": True,\n",
    "                \"config_id\": config_id,\n",
    "                \"create_config\": {\n",
    "                    \"comment\": \"YOUR_CONFIGURATION_COMMENT\",\n",
    "                    \"model_id\": model_id,\n",
    "                    # \"model_version_number\": \"\"\n",
    "                },\n",
    "                \"device_ids\": [device_id],\n",
    "                # \"replace_model_id\": \"\",\n",
    "                \"comment\": \"YOUR_MODEL_DEPLOY_COMMENT\",\n",
    "            },\n",
    "            \"deploy_app\": {\n",
    "                \"app_name\": app_name,\n",
    "                \"version_number\": app_version_number,\n",
    "                \"device_ids\": [device_id],\n",
    "                \"comment\": \"YOUR_APP_DEPLOY_COMMENT\",\n",
    "            },\n",
    "            \"command_parameter_file_name\": command_parameter_file_name\n",
    "        }\n",
    "        json.dump(configuration, fp, indent=4)\n",
    "    print(\"./configuration.json is created.\")\n",
    "else:\n",
    "    print(\"./configuration.json already exists.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit Configuration file\n",
    "\n",
    "Edit the parameters in [configuration.json](./configuration.json).\n",
    "\n",
    "|Setting|Description|Range|Required/Optional|Remarks\n",
    "|:--|:--|:--|:--|:--|\n",
    "|**`import_model`**||Object.<br>See following table of import_model|Required||\n",
    "|**`import_app`**||Object.<br>See following table of import_app|Required||\n",
    "|**`deploy_model`**||Object.<br>See following table of deploy_model|Required||\n",
    "|**`deploy_app`**||Object.<br>See following table of deploy_app|Required||\n",
    "|**`command_parameter_file_name`**|The filename of the Command Parameter you want to save.|String.|Required|Saving filename in the SDK.<br>The file is used for uploading to \"**Console for AITRIOS**\"|\n",
    "\n",
    "##### import_model\n",
    "\n",
    "|Setting|Description|Range|Required/Optional|Remarks\n",
    "|:--|:--|:--|:--|:--|\n",
    "|**`model_id`**|The ID of the AI model you want to import|String. <br>See NOTE. |Required|Used for \"**Console Access Library**\" API:<br>**`ai_model.ai_model.AIModel.import_base_model`**<br>**`ai_model.ai_model.AIModel.get_base_model_status`**<br>**`ai_model.ai_model.AIModel.publish_model`** |\n",
    "|**`model`**|Path to SAS URI for AI model|SAS URI. <br>See NOTE. |Required|Used for \"**Console Access Library**\" API:<br>**`ai_model.ai_model.AIModel.import_base_model`**|\n",
    "|**`vendor_name`**|vendor name|String. <br>See NOTE. |Optional|Used for \"**Console Access Library**\" API:<br>**`ai_model.ai_model.AIModel.import_base_model`**|\n",
    "|**`comment`**|Description of the AI model and version|String. <br>See NOTE. |Optional|Used for \"**Console Access Library**\" API:<br>**`ai_model.ai_model.AIModel.import_base_model`**|\n",
    "|**`labels`**|Label names|[\"label01\", \"label02\", ...]<br>See NOTE. |Optional|Used for \"**Console Access Library**\" API:<br>**`ai_model.ai_model.AIModel.import_base_model`**|\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> See [API Reference](https://developer.aitrios.sony-semicon.com/development-guides/reference/api-references/) of Console Access Library for other restrictions.\n",
    "\n",
    "##### import_app\n",
    "\n",
    "|Setting|Description|Range|Required/Optional|Remarks\n",
    "|:--|:--|:--|:--|:--|\n",
    "|**`ppl_file`**|The path to the \"**Vision and Sensing Application**\" file for importing |Absolute path or relative path from configuration.json/Notebook(*.ipynb)|Required||\n",
    "|**`app_name`**|The name of the \"**Vision and Sensing Application**\" you want to import|String. <br>See NOTE. |Required|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.import_device_app`**|\n",
    "|**`version_number`**|The version number of the \"**Vision and Sensing Application**\" you want to import |String. <br>See NOTE. |Required|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.import_device_app`** |\n",
    "|**`comment`**|The description of the \"**Vision and Sensing Application**\" |String. <br>See NOTE. |Optional|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.import_device_app`** |\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> See [API Reference](https://developer.aitrios.sony-semicon.com/development-guides/reference/api-references/) of Console Access Library for other restrictions.\n",
    "\n",
    "##### deploy_model\n",
    "\n",
    "|Setting||Description|Range|Required/Optional|Remarks\n",
    "|:--|:--|:--|:--|:--|:--|\n",
    "|**`should_create_deploy_config`**||Whether to register a new deploy configuration.<br>If true, create a new configuration; if false, use an already registered configuration specified by **`config_id`**.|true or false|Required||\n",
    "|**`config_id`**||The ID of the deploy configuration you want to use for deployment.|String.<br>See NOTE.|Required|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.create_deploy_configuration`**<br>**`deployment.deployment.Deployment.deploy_by_configuration`**|\n",
    "|**`create_config`**|**`comment`**|Description of the configuration.|String. <br>See NOTE.|Optional|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.create_deploy_configuration`**|\n",
    "||**`model_id`**|The ID of the model you want to deploy.|String.<br>See NOTE.|Optional<br> Required if **`should_create_deploy_config`** is true.|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.create_deploy_configuration`**|\n",
    "||**`model_version_number`**|The version of the model you want to deploy.|String.<br>See NOTE.|Optional|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.create_deploy_configuration`**|\n",
    "|**`device_ids`**||List of device ids on which you want to deploy your model.|List of string.|Required|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.deploy_by_configuration`**|\n",
    "|**`replace_model_id`**||The ID of the model you want to replace.|String.<br>See NOTE.|Optional|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.deploy_by_configuration`**|\n",
    "|**`comment`**||Description of the deployment.|String.<br>See NOTE.|Optional|Used for \"**Console Access Library**\" API:<br>**`deployment.deployment.Deployment.deploy_by_configuration`**|\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> See [API Reference](https://developer.aitrios.sony-semicon.com/development-guides/reference/api-references/) of Console Access Library for other restrictions.\n",
    "\n",
    "##### deploy_app\n",
    "\n",
    "|Setting|Description|Range|Required/Optional|Remarks\n",
    "|:--|:--|:--|:--|:--|\n",
    "|**`app_name`**|The name of the \"**Vision and Sensing Application**\" you want to deploy.|String.<br>See NOTE.|Required|Used for \"**Console Access Library**\" API: <br> **`deployment.deployment.Deployment.deploy_device_app`** <br> **`deployment.deployment.Deployment.get_device_app_deploys`**|\n",
    "|**`version_number`**|The version number of the \"**Vision and Sensing Application**\" you want deploy.|String.<br>See NOTE.|Required|Used for \"**Console Access Library**\" API: <br> **`deployment.deployment.Deployment.deploy_device_app`** <br> **`deployment.deployment.Deployment.get_device_app_deploys`**|\n",
    "|**`device_ids`**|List of device ids on which you want to deploy your \"**Vision and Sensing Application**\".|List of strings.|Required|Used for \"**Console Access Library**\" API: <br> **`deployment.deployment.Deployment.deploy_device_app`**|\n",
    "|**`comment`**|Description of the deployment.|String.<br>See NOTE.|Optional|Used for \"**Console Access Library**\" API: <br> **`deployment.deployment.Deployment.deploy_device_app`**|\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> See [API Reference](https://developer.aitrios.sony-semicon.com/development-guides/reference/api-references/) of Console Access Library for other restrictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configurations (for importing and deploying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = Path(\"./configuration.json\")\n",
    "validate_symlink(configuration_path)\n",
    "\n",
    "# Load configuration file.\n",
    "with open(configuration_path, \"r\") as f:\n",
    "    json_load = json.load(f)\n",
    "\n",
    "configuration_schema_path = Path(\"./configuration_schema.json\")\n",
    "validate_symlink(configuration_schema_path)\n",
    "\n",
    "# Load configuration schema file.\n",
    "with open(configuration_schema_path, \"r\") as f:\n",
    "    json_schema = json.load(f)\n",
    "\n",
    "# Validate configuration.\n",
    "jsonschema.validate(json_load, json_schema)\n",
    "\n",
    "IMPORT_MODEL_SCHEMA_PATH = (\n",
    "    f\"{SDK_ENV_ROOT_DIR}/tutorials/3_prepare_model/develop_on_sdk/\"\n",
    "    \"3_import_to_console/configuration_schema.json\"\n",
    ")\n",
    "IMPORT_APP_SCHEMA_PATH = (\n",
    "    f\"{SDK_ENV_ROOT_DIR}/tutorials/4_prepare_application/\"\n",
    "    \"2_import_to_console/configuration_schema.json\"\n",
    ")\n",
    "DEPLOY_MODEL_SCHEMA_PATH = (\n",
    "    f\"{SDK_ENV_ROOT_DIR}/tutorials/3_prepare_model/develop_on_sdk/\"\n",
    "    \"4_deploy_to_device/deploy_to_device/configuration_schema.json\"\n",
    ")\n",
    "DEPLOY_APP_SCHEMA_PATH = (\n",
    "    f\"{SDK_ENV_ROOT_DIR}/tutorials/4_prepare_application/\"\n",
    "    \"3_deploy_to_device/configuration_schema.json\"\n",
    ")\n",
    "\n",
    "\n",
    "def validate_objects(schema_path: str, key: str):\n",
    "    configuration_schema_path = Path(schema_path)\n",
    "    validate_symlink(configuration_schema_path)\n",
    "    # Load configuration schema file.\n",
    "    with open(configuration_schema_path, \"r\") as f:\n",
    "        json_schema = json.load(f)\n",
    "    # Validate configuration.\n",
    "    jsonschema.validate(json_load[key], json_schema)\n",
    "\n",
    "\n",
    "validate_objects(IMPORT_MODEL_SCHEMA_PATH, \"import_model\")\n",
    "validate_objects(IMPORT_APP_SCHEMA_PATH, \"import_app\")\n",
    "validate_objects(DEPLOY_MODEL_SCHEMA_PATH, \"deploy_model\")\n",
    "validate_objects(DEPLOY_APP_SCHEMA_PATH, \"deploy_app\")\n",
    "\n",
    "print(\"./configuration.json is loaded.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import AI model to \"**Console**\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model(json_load):\n",
    "    \"\"\"Import AI model to Console\"\"\"\n",
    "\n",
    "    model_id = json_load[\"model_id\"]\n",
    "    model_import_param = {}\n",
    "    model_import_param[\"model_id\"] = model_id\n",
    "    model_import_param[\"model\"] = json_load[\"model\"]\n",
    "    model_import_param[\"network_type\"] = \"0\"\n",
    "    if \"vendor_name\" in json_load:\n",
    "        model_import_param[\"vendor_name\"] = json_load[\"vendor_name\"]\n",
    "    if \"comment\" in json_load:\n",
    "        model_import_param[\"comment\"] = json_load[\"comment\"]\n",
    "    if \"labels\" in json_load:\n",
    "        model_import_param[\"labels\"] = copy.deepcopy(json_load[\"labels\"])\n",
    "\n",
    "    # Call an API to import AI model into Console for AITRIOS\n",
    "    try:\n",
    "        ai_model_obj = client_obj.get_ai_model()\n",
    "        # print(model_import_param)\n",
    "        response = ai_model_obj.import_base_model(**model_import_param)\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "    print(\"Start to import the AI model.\" + \" \\n\\tmodel_id: \" + model_id)\n",
    "\n",
    "\n",
    "import_model(json_load[\"import_model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model(json_load):\n",
    "    \"\"\"Convert AI model on Console\"\"\"\n",
    "\n",
    "    model_id = json_load[\"model_id\"]\n",
    "    # Call an API to convert AI model on Console for AITRIOS\n",
    "    try:\n",
    "        ai_model_obj = client_obj.get_ai_model()\n",
    "        response = ai_model_obj.publish_model(model_id=model_id)\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "    print(\"Start to convert the AI model.\" + \" \\n\\tmodel_id: \" + model_id)\n",
    "\n",
    "\n",
    "convert_model(json_load[\"import_model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check AI model status after conversion\n",
    "\n",
    "To complete the conversion, ensure that the conversion status is **`Conversion completed`**.\n",
    "\n",
    "After you start the conversion, run the following code cell to check the status.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> Conversion takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model_status(json_load):\n",
    "    \"\"\"Get AI model status on Console\"\"\"\n",
    "\n",
    "    model_id = json_load[\"model_id\"]\n",
    "    # AI model status on Console\n",
    "    status_dictionary = {\n",
    "        \"00\": \"Conversion project created\",\n",
    "        \"01\": \"Importing completed (Before conversion)\",\n",
    "        \"02\": \"Converting...\",\n",
    "        \"03\": \"Conversion failed\",\n",
    "        \"04\": \"Converted\",\n",
    "        \"05\": \"Adding to configuration\",\n",
    "        \"06\": \"Conversion failed\",\n",
    "        \"07\": \"Conversion completed\",\n",
    "        \"11\": \"Saving\",\n",
    "    }\n",
    "    # Flag for check\n",
    "    exist_flag = False\n",
    "    # Call an API for get AI model info\n",
    "    try:\n",
    "        ai_model_obj = client_obj.get_ai_model()\n",
    "        # print(model_id)\n",
    "        response = ai_model_obj.get_base_model_status(model_id)\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "    # Create output list\n",
    "    if \"projects\" in response:\n",
    "        project = response[\"projects\"][0]\n",
    "        if \"versions\" in project:\n",
    "            version_status = project[\"versions\"][0][\"version_status\"]\n",
    "            exist_flag = True\n",
    "\n",
    "    if exist_flag:\n",
    "        message = status_dictionary.get(\n",
    "            version_status, \"Unknown status '\" + version_status + \"'\"\n",
    "        )\n",
    "        return (version_status, message)\n",
    "    else:\n",
    "        raise Exception(\"AI model is not found. (model_id: \" + model_id + \")\")\n",
    "\n",
    "\n",
    "status, message = get_base_model_status(json_load[\"import_model\"])\n",
    "print(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import \"**Vision and Sensing Application**\" to \"**Console**\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import \"**Vision and Sensing Application**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_app(json_load):\n",
    "    \"\"\"Import Vision and Sensing Application to Console\"\"\"\n",
    "\n",
    "    # encode base 64\n",
    "    def convert_file_to_b64_string(file_path):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read())\n",
    "\n",
    "    ppl_file = Path(json_load[\"ppl_file\"].replace(os.path.sep, \"/\"))\n",
    "    validate_symlink(ppl_file)\n",
    "\n",
    "    file_content = convert_file_to_b64_string(ppl_file)\n",
    "\n",
    "    file_name = os.path.basename(ppl_file)\n",
    "\n",
    "    print(json_load[\"ppl_file\"] + \" is loaded.\")\n",
    "\n",
    "    vasa_import_param = {}\n",
    "    vasa_import_param[\"app_name\"] = json_load[\"app_name\"]\n",
    "    vasa_import_param[\"version_number\"] = json_load[\"version_number\"]\n",
    "    vasa_import_param[\"compiled_flg\"] = \"0\"\n",
    "    vasa_import_param[\"entry_point\"] = \"main\"\n",
    "    vasa_import_param[\"file_name\"] = file_name\n",
    "    vasa_import_param[\"file_content\"] = file_content\n",
    "    if \"comment\" in json_load:\n",
    "        vasa_import_param[\"comment\"] = json_load[\"comment\"]\n",
    "\n",
    "    # Call an API to import Vision and Sensing Application into Console for AITRIOS\n",
    "    try:\n",
    "        deployment_obj = client_obj.get_deployment()\n",
    "        # print(vasa_import_param)\n",
    "        response = deployment_obj.import_device_app(**vasa_import_param)\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "    print(\n",
    "        \"Start to import the Vision and Sensing Application.\"\n",
    "        + \" \\n\\tapp_name: \"\n",
    "        + json_load[\"app_name\"]\n",
    "        + \"\\n\\tversion_number: \"\n",
    "        + json_load[\"version_number\"]\n",
    "    )\n",
    "\n",
    "\n",
    "import_app(json_load[\"import_app\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check \"**Vision and Sensing Application**\" status\n",
    "\n",
    "To complete the conversion, ensure that the conversion status is **`Conversion completed`**.\n",
    "\n",
    "After you start the conversion, run the following code cell to check the status.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Import and conversion take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_app_status(json_load):\n",
    "    \"\"\"Get Vision and Sensing Application status on Console\"\"\"\n",
    "\n",
    "    app_name = json_load[\"app_name\"]\n",
    "    version_number = json_load[\"version_number\"]\n",
    "\n",
    "    # Status of Vision and Sensing Application on Console\n",
    "    status_dictionary = {\n",
    "        \"0\": \"Importing completed (Before conversion)\",\n",
    "        \"1\": \"Converting...\",\n",
    "        \"2\": \"Conversion completed\",\n",
    "        \"3\": \"Conversion failed\",\n",
    "    }\n",
    "    # Flag for import check\n",
    "    import_flag = False\n",
    "    # Call an API to get Vision and Sensing Application info\n",
    "    try:\n",
    "        deployment_obj = client_obj.get_deployment()\n",
    "        response = deployment_obj.get_device_apps()\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "    # Create output list\n",
    "    apps = response.get(\"apps\", [])\n",
    "    for app in apps:\n",
    "        if \"name\" in app and app[\"name\"] == app_name:\n",
    "            versions = app.get(\"versions\", [])\n",
    "            for version in versions:\n",
    "                if \"version\" in version and version[\"version\"] == version_number:\n",
    "                    import_flag = True\n",
    "                    version_status = version.get(\"status\", \"\")\n",
    "                    break\n",
    "            if import_flag:\n",
    "                break\n",
    "    if import_flag:\n",
    "        return status_dictionary.get(\n",
    "            version_status, \"Unknown status '\" + version_status + \"'\"\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Vision and Sensing Application is not found. \"\n",
    "            + \" \\n\\tapp_name: \"\n",
    "            + app_name\n",
    "            + \"\\n\\tversion_number: \"\n",
    "            + version_number\n",
    "        )\n",
    "\n",
    "\n",
    "def check_app_status(json_load):\n",
    "    \"\"\"Check Vision and Sensing Application status on Console\"\"\"\n",
    "\n",
    "    get_status = get_device_app_status(json_load)\n",
    "    print(\n",
    "        get_status\n",
    "        + \" \\n\\tapp_name: \"\n",
    "        + json_load[\"app_name\"]\n",
    "        + \"\\n\\tversion_number: \"\n",
    "        + json_load[\"version_number\"]\n",
    "    )\n",
    "\n",
    "\n",
    "check_app_status(json_load[\"import_app\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create deploy configuration in \"**Console**\"\n",
    "\n",
    "To deploy AI model to Edge AI Device, deploy configuration includes information of AI model and Edge AI Device to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deploy_config(json_load):\n",
    "    \"\"\"Create deploy configuration on Console\"\"\"\n",
    "\n",
    "    # Set values\n",
    "    should_create_deploy_config = json_load[\"should_create_deploy_config\"]\n",
    "\n",
    "    if should_create_deploy_config is True:\n",
    "        config_param = {}\n",
    "        config_param[\"config_id\"] = json_load[\"config_id\"]\n",
    "        if \"create_config\" in json_load:\n",
    "            config_param[\"model_id\"] = json_load[\"create_config\"][\"model_id\"]\n",
    "            if \"model_version_number\" in json_load[\"create_config\"]:\n",
    "                config_param[\"model_version_number\"] = json_load[\"create_config\"][\n",
    "                    \"model_version_number\"\n",
    "                ]\n",
    "            if \"comment\" in json_load[\"create_config\"]:\n",
    "                config_param[\"comment\"] = json_load[\"create_config\"][\"comment\"]\n",
    "\n",
    "    if should_create_deploy_config is True:\n",
    "        # Call an API to create deploy configuration\n",
    "        try:\n",
    "            deployment_obj = client_obj.get_deployment()\n",
    "            # print(config_param)\n",
    "            response = deployment_obj.create_deploy_configuration(**config_param)\n",
    "        except Exception as e:\n",
    "            # EXCEPTION\n",
    "            raise e\n",
    "\n",
    "        # response error check\n",
    "        if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "            # ERROR\n",
    "            raise ValueError(\"ERROR\", response)\n",
    "\n",
    "        # SUCCESS\n",
    "        print(\"Deploy configuration was created.\")\n",
    "\n",
    "\n",
    "create_deploy_config(json_load[\"deploy_model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model to device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(json_load):\n",
    "    \"\"\"Deploy AI model to device\"\"\"\n",
    "\n",
    "    device_ids_list = json_load[\"device_ids\"]\n",
    "\n",
    "    # Set values\n",
    "    deploy_param = {}\n",
    "    deploy_param[\"config_id\"] = json_load[\"config_id\"]\n",
    "    deploy_param[\"device_ids\"] = \",\".join(device_ids_list)\n",
    "    if \"replace_model_id\" in json_load:\n",
    "        deploy_param[\"replace_model_id\"] = json_load[\"replace_model_id\"]\n",
    "    if \"comment\" in json_load:\n",
    "        deploy_param[\"comment\"] = json_load[\"comment\"]\n",
    "\n",
    "    # Call an API to deploy the model to device\n",
    "    try:\n",
    "        deployment_obj = client_obj.get_deployment()\n",
    "        # print(deploy_param)\n",
    "        response = deployment_obj.deploy_by_configuration(**deploy_param)\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "    print(\"Start to deploy the model.\")\n",
    "\n",
    "\n",
    "deploy_model(json_load[\"deploy_model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check deployment status\n",
    "\n",
    "To complete the deployment, ensure that the deployment status is **`Success`**.\n",
    "\n",
    "After you start the deployment, run the following code cell to check the status.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> Deploying to devices takes several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_deploy_status(json_load):\n",
    "    \"\"\"Get and check deploy status of AI model\"\"\"\n",
    "\n",
    "    device_ids_list: list = json_load[\"device_ids\"]\n",
    "    deploy_config_id: str = json_load[\"config_id\"]\n",
    "\n",
    "    # Deploy status on Console\n",
    "    deploy_status_dictionary = {\n",
    "        \"0\": \"Deploying\",\n",
    "        \"1\": \"Success\",\n",
    "        \"2\": \"Fail\",\n",
    "        \"3\": \"Cancel\",\n",
    "    }\n",
    "    # Model status on Console\n",
    "    model_status_dictionary = {\n",
    "        \"0\": \"Waiting for execution\",\n",
    "        \"1\": \"Deploying\",\n",
    "        \"2\": \"Success\",\n",
    "        \"3\": \"Fail\",\n",
    "    }\n",
    "\n",
    "    deploy_ids = []\n",
    "    config_ids = []\n",
    "    deploy_statuses = []\n",
    "    model_statuses = []\n",
    "    update_dates = []\n",
    "    device_id_table = []\n",
    "    for device_id in device_ids_list:\n",
    "        # Call an API to get deploy history\n",
    "        try:\n",
    "            deployment_obj = client_obj.get_deployment()\n",
    "            response = deployment_obj.get_deploy_history(device_id)\n",
    "        except Exception as e:\n",
    "            # EXCEPTION\n",
    "            raise e\n",
    "\n",
    "        # response error check\n",
    "        if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "            # ERROR\n",
    "            raise ValueError(\"ERROR\", response)\n",
    "\n",
    "        # Create an output table\n",
    "        deploys = response.get(\"deploys\", [])\n",
    "        cnt = 0\n",
    "        for deploy in deploys:\n",
    "            model = deploy.get(\"model\", {})\n",
    "            model_target_flg = model.get(\"model_target_flg\", \"\")\n",
    "            config_id = deploy.get(\"config_id\", \"\")\n",
    "            if model_target_flg == \"1\" and deploy_config_id == config_id:\n",
    "                # Set device id\n",
    "                if cnt == 0:\n",
    "                    device_id_table.append(device_id)\n",
    "                else:\n",
    "                    # Fill a cell with a NAN\n",
    "                    device_id_table.append(np.NaN)\n",
    "                # Set deploy ID\n",
    "                deploy_id = deploy.get(\"id\", \"\")\n",
    "                deploy_ids.append(deploy_id)\n",
    "                # Set config ID\n",
    "                config_ids.append(config_id)\n",
    "                # Set deploy status\n",
    "                deploy_status = deploy.get(\"deploy_status\", \"\")\n",
    "                deploy_statuses.append(\n",
    "                    deploy_status_dictionary.get(\n",
    "                        deploy_status, \"Unknown status '\" + deploy_status + \"'\"\n",
    "                    )\n",
    "                )\n",
    "                # Set model status\n",
    "                model_status = model.get(\"model_status\", \"\")\n",
    "                model_statuses.append(\n",
    "                    model_status_dictionary.get(\n",
    "                        model_status, \"Unknown status '\" + model_status + \"'\"\n",
    "                    )\n",
    "                )\n",
    "                # Set update date\n",
    "                update_dates.append(deploy.get(\"upd_date\", \"\"))\n",
    "\n",
    "                cnt += 1\n",
    "                # Display up to 5 deployment results\n",
    "                if cnt == 5:\n",
    "                    break\n",
    "\n",
    "    if len(deploy_ids) == 0:\n",
    "        raise Exception(\"There is no data in the deploy history list.\")\n",
    "\n",
    "    output_frame = pd.DataFrame(\n",
    "        {\n",
    "            \"device_id\": device_id_table,\n",
    "            \"deploy_id\": deploy_ids,\n",
    "            \"config_id\": config_ids,\n",
    "            \"deploy_status\": deploy_statuses,\n",
    "            \"model_status\": model_statuses,\n",
    "            \"update_date\": update_dates,\n",
    "        }\n",
    "    )\n",
    "    output_frame = output_frame.fillna(\"-\")\n",
    "    # setting backup\n",
    "    backup_max_rows = pd.options.display.max_rows\n",
    "    # output limit clear\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    display(output_frame)\n",
    "    # setting restore\n",
    "    pd.set_option(\"display.max_rows\", backup_max_rows)\n",
    "\n",
    "\n",
    "check_deploy_status(json_load[\"deploy_model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy \"**Vision and Sensing Application**\" to device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy \"**Vision and Sensing Application**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_app(json_load):\n",
    "    \"\"\"Deploy Vision and Sensing Application to device\"\"\"\n",
    "\n",
    "    param_dict = json_load.copy()\n",
    "\n",
    "    device_ids = param_dict[\"device_ids\"]\n",
    "    device_ids_join = \",\".join(device_ids)\n",
    "    param_dict[\"device_ids\"] = device_ids_join\n",
    "\n",
    "    # Call an API to deploy Vision and Sensing Application from Console for AITRIOS to Edge AI\n",
    "    # Device.\n",
    "    try:\n",
    "        deployment_obj = client_obj.get_deployment()\n",
    "        # print(param_dict)\n",
    "        response = deployment_obj.deploy_device_app(**param_dict)\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # Response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "    print(\n",
    "        \"Start to deploy application. \\n\\tapp_name: \"\n",
    "        + json_load[\"app_name\"]\n",
    "        + \"\\n\\tversion_number: \"\n",
    "        + json_load[\"version_number\"]\n",
    "        + \"\\n\\tdevice_ids: \"\n",
    "        + device_ids_join\n",
    "    )\n",
    "\n",
    "\n",
    "deploy_app(json_load[\"deploy_app\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check deployment status\n",
    "\n",
    "To complete the deployment, ensure that the deployment status is **`Success`**.\n",
    "\n",
    "After you start the deployment, run the following code cell to check the status.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> Deploying to devices takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_app_deploy_status(json_load):\n",
    "    \"\"\"Check deploy status of Vision and Sensing Application\"\"\"\n",
    "\n",
    "    device_ids = json_load[\"device_ids\"]\n",
    "\n",
    "    status_dictionary = {\"0\": \"Deploying\", \"1\": \"Success\", \"2\": \"Fail\", \"3\": \"Cancel\"}\n",
    "\n",
    "    response_statuses = []\n",
    "    response_device_ids = []\n",
    "\n",
    "    # Call an API to get Vision and Sensing Application info.\n",
    "    try:\n",
    "        deployment_obj = client_obj.get_deployment()\n",
    "        response = deployment_obj.get_device_app_deploys(\n",
    "            json_load[\"app_name\"], json_load[\"version_number\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # EXCEPTION\n",
    "        raise e\n",
    "\n",
    "    # response error check\n",
    "    if \"result\" in response and response[\"result\"] != \"SUCCESS\":\n",
    "        # ERROR\n",
    "        raise ValueError(\"ERROR\", response)\n",
    "\n",
    "    # SUCCESS\n",
    "\n",
    "    deploys = response.get(\"deploys\", [])\n",
    "    # Display deployment status of specified devices\n",
    "    for deploy in deploys:\n",
    "        if \"devices\" in deploy:\n",
    "            devices = deploy.get(\"devices\", [])\n",
    "            for device in devices:\n",
    "                if (\n",
    "                    \"device_id\" in device\n",
    "                    and device[\"device_id\"] in device_ids\n",
    "                    and device[\"latest_deployment_flg\"] == \"1\"\n",
    "                ):\n",
    "                    response_device_ids.append(device.get(\"device_id\", \"\"))\n",
    "                    deploy_status = device.get(\"status\", \"\")\n",
    "                    response_statuses.append(\n",
    "                        status_dictionary.get(\n",
    "                            deploy_status, \"Unknown status '\" + deploy_status + \"'\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    if len(response_device_ids) == 0:\n",
    "        raise Exception(\n",
    "            \"Failed to get deploy status. Deploy history not found. \\n\\tapp_name: \"\n",
    "            + json_load[\"app_name\"]\n",
    "            + \"\\n\\tversion_number: \"\n",
    "            + json_load[\"version_number\"]\n",
    "            + \"\\n\\tdevice_ids: \"\n",
    "            + \",\".join(device_ids)\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"Deployment status of: \\n\\tapp_name: \"\n",
    "        + json_load[\"app_name\"]\n",
    "        + \"\\n\\tversion_number: \"\n",
    "        + json_load[\"version_number\"]\n",
    "    )\n",
    "\n",
    "    output_frame = pd.DataFrame(\n",
    "        {\"device_id\": response_device_ids, \"status\": response_statuses}\n",
    "    )\n",
    "\n",
    "    # setting backup\n",
    "    backup_max_rows = pd.options.display.max_rows\n",
    "    # output limit clear\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    # View results\n",
    "    display(output_frame)\n",
    "    # setting restore\n",
    "    pd.set_option(\"display.max_rows\", backup_max_rows)\n",
    "\n",
    "\n",
    "check_app_deploy_status(json_load[\"deploy_app\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate without Web App\n",
    "Please see [Console User Manual](https://developer.aitrios.sony-semicon.com/en/documents/console-user-manual) for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Command Parameter File\n",
    "\n",
    "Before running following cell, edit following **`model_id`**, **`ppl_parameter`** and **`command_parameter`** if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = json_load[\"import_model\"][\"model_id\"]\n",
    "\n",
    "ppl_parameter = {\n",
    "    \"header\": {\n",
    "        \"id\": \"00\",\n",
    "        \"version\": APP_VERSION_NUMBER\n",
    "    },\n",
    "    \"dnn_output_detections\": DNN_OUTPUT_DETECTIONS,\n",
    "    \"max_detections\": MAX_DETECTIONS,\n",
    "    \"mode\": 0,\n",
    "    \"zone\": {\n",
    "        \"top_left_x\": 10,\n",
    "        \"top_left_y\": 168,\n",
    "        \"bottom_right_x\": 198,\n",
    "        \"bottom_right_y\": 258\n",
    "    },\n",
    "    \"threshold\": {\n",
    "        \"iou\": 0.5,\n",
    "        \"score\": 0.3\n",
    "    },\n",
    "    \"input_width\": INPUT_SIZE,\n",
    "    \"input_height\": INPUT_SIZE\n",
    "}\n",
    "\n",
    "command_parameter = {\n",
    "    \"commands\": [\n",
    "        {\n",
    "            \"command_name\": \"StartUploadInferenceData\",\n",
    "            \"parameters\": {\n",
    "                \"Mode\": 1,  # 0: image only, 1: image and inference, 2: inference only, default: 0\n",
    "                \"NumberOfImages\": 1,  # default: 0, 0 means uploading images until stop\n",
    "                \"PPLParameter\": ppl_parameter,\n",
    "                \"ModelId\": model_id,\n",
    "                # \"UploadInterval\": 30, # 30-2592000 default: 30 (a unit is 1/30 seconds)\n",
    "                # \"UploadMethod\": \"BlobStorage\", # default: \"BlobStorage\"\n",
    "                # \"FileFormat\": \"JPG\", # \"JPG\", \"BMP\" default: \"JPG\"\n",
    "                # \"UploadMethodIR\": \"Mqtt\", # default: \"Mqtt\"\n",
    "                # \"NumberOfInferencesPerMessage\": 1, # 1-100 default: 1\n",
    "                # \"MaxDetectionsPerFrame\": 5, # 1-5 default: 5\n",
    "                # \"CropHOffset\": 0, # 0-4055 default: 0\n",
    "                # \"CropVOffset\": 0, # 0-3039 default: 0\n",
    "                # \"CropHSize\": 4056, # 0-4056 default: 4056\n",
    "                # \"CropVSize\": 3040, # 0-3040 default: 3040\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "command_parameter_file_name = json_load[\"command_parameter_file_name\"]\n",
    "\n",
    "with open(command_parameter_file_name, \"w\") as fp:\n",
    "    json.dump(command_parameter, fp, indent=4)\n",
    "    print(\n",
    "        f\"Command Parameter File of StartUploadInferenceData \\\n",
    "            is saved at ./{command_parameter_file_name}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Command Parameter File of StartUploadInferenceData including PPL Parameter\n",
    "\n",
    "Upload the saved Command Parameter File from \"**Settings**\" in \"**Console for AITRIOS**\" Web UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bind the Command Parameter File to Edge AI Device\n",
    "\n",
    "Bind the Command Parameter File  from \"**Manage device**\" in \"**Console for AITRIOS**\" Web UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute StartUploadInferenceData\n",
    "\n",
    "Start inference from \"**Manage device**\" in \"**Console for AITRIOS**\" Web UI.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> If there is no subject of images, the correct inference results will not be obtained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute StopUploadInferenceData\n",
    "\n",
    "Stop inference from \"**Manage device**\" in \"**Console for AITRIOS**\" Web UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check image and metadata of inference result\n",
    "\n",
    "Check images and inference results from \"**Check data**\" in \"**Console for AITRIOS**\" Web UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with Web App\n",
    "\n",
    "You can run a web application of [\"**Zone Detection**\"](https://developer.aitrios.sony-semicon.com/development-guides/tutorials/sample-application/) sample application outside the SDK environment.\n",
    "\n",
    "If you use the web application, you can use the Wasm file of \"**Vision and Sensing Application**\" created with this notebook.\n",
    "\n",
    "> **Note**\n",
    "> \n",
    "> The web application is made for using dataset annotated in \"**Console for AITRIOS**\" and AI model trained in \"**Console for AITRIOS**\".\n",
    ">\n",
    "> So following PPL Parameter is used in the web application.\n",
    "> \n",
    "> - **`input_size`** is 320\n",
    "> - **`dnn_output_detections`** is 64\n",
    ">\n",
    "> The dataset annotated in \"**Console for AITRIOS**\" can't be exported to the SDK environment.\n",
    ">\n",
    "> The AI model trained in \"**Console for AITRIOS**\" can't be exported to the SDK environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
